{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mike/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mike/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Get data from file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update([',','\\'','.','\\\"','...','`','#','$','%','&','*',';',':','/b','u','gt','lt','//','\\'s','\\'\\'','-','reuter'])\n",
    "classes = np.array([1,2,3,4])\n",
    "training_filename = 'ag_news_csv/train.csv'\n",
    "testing_filename = 'ag_news_csv/test.csv'\n",
    "col_names = ['class','title','description']\n",
    "training = pd.read_csv(training_filename, names=col_names)\n",
    "testing = pd.read_csv(testing_filename, names=col_names)\n",
    "\n",
    "tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "rgx_list = ['(\\w+[A-Z]+.*\\-+\\s)','(\\({1}\\w+[\\.*\\s*\\w*]*\\){1})']\n",
    "\n",
    "def token_stem_lem(text):\n",
    "    # remove location of article before first - , and any source denoted by parenthesis\n",
    "    new_text = text\n",
    "    for rgx_match in rgx_list:\n",
    "        new_text = re.sub(rgx_match, '', new_text)\n",
    "    words_stemmed_lemmed = []\n",
    "    for word in tokenizer.tokenize(new_text):\n",
    "        new_word = stemmer.stem(lemmer.lemmatize(word.lower()))\n",
    "        if new_word not in stop_words :\n",
    "            words_stemmed_lemmed.append(new_word)\n",
    "    return words_stemmed_lemmed\n",
    "\n",
    "def to_onehot(y, class_rng):\n",
    "    res = []\n",
    "    for i in class_rng:\n",
    "        if y is i:\n",
    "            res.append(1)\n",
    "        else:\n",
    "            res.append(0)\n",
    "    return res\n",
    "\n",
    "testing['title_proc'] = testing['title'].apply(lambda title: token_stem_lem(title))\n",
    "testing['descrip_proc'] = testing['description'].apply(lambda desc: token_stem_lem(desc))\n",
    "testing['onehot'] = testing['class'].apply(lambda y: to_onehot(y, range(1, classes.shape[0] + 1)))\n",
    "training['title_proc'] = training['title'].apply(lambda title: token_stem_lem(title))\n",
    "training['descrip_proc'] = training['description'].apply(lambda desc: token_stem_lem(desc))\n",
    "training['onehot'] = training['class'].apply(lambda y: to_onehot(y, range(1, classes.shape[0] + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>title_proc</th>\n",
       "      <th>descrip_proc</th>\n",
       "      <th>onehot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "      <td>[wall, st., bear, claw, back, black]</td>\n",
       "      <td>[short-sel, wall, street, dwindling\\band, ultr...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>[carlyl, look, toward, commerci, aerospac]</td>\n",
       "      <td>[privat, invest, firm, carlyl, group, \\which, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>[oil, economi, cloud, stock, outlook]</td>\n",
       "      <td>[soar, crude, price, plu, worries\\about, econo...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "      <td>[iraq, halt, oil, export, main, southern, pipe...</td>\n",
       "      <td>[author, halt, oil, export\\flow, main, pipelin...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "      <td>[oil, price, soar, all-tim, record, pose, new,...</td>\n",
       "      <td>[tearaway, world, oil, price, toppl, record, s...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1</td>\n",
       "      <td>Pakistan's Musharraf Says Won't Quit as Army C...</td>\n",
       "      <td>KARACHI (Reuters) - Pakistani President Perve...</td>\n",
       "      <td>[pakistan, musharraf, say, wo, n't, quit, armi...</td>\n",
       "      <td>[pakistani, presid, pervez, musharraf, ha, sai...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>2</td>\n",
       "      <td>Renteria signing a top-shelf deal</td>\n",
       "      <td>Red Sox general manager Theo Epstein acknowled...</td>\n",
       "      <td>[renteria, sign, top-shelf, deal]</td>\n",
       "      <td>[red, sox, gener, manag, theo, epstein, acknow...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>2</td>\n",
       "      <td>Saban not going to Dolphins yet</td>\n",
       "      <td>The Miami Dolphins will put their courtship of...</td>\n",
       "      <td>[saban, go, dolphin, yet]</td>\n",
       "      <td>[miami, dolphin, put, courtship, lsu, coach, n...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>2</td>\n",
       "      <td>Today's NFL games</td>\n",
       "      <td>PITTSBURGH at NY GIANTS Time: 1:30 p.m. Line: ...</td>\n",
       "      <td>[today, nfl, game]</td>\n",
       "      <td>[pittsburgh, ny, giant, time, 1:30, p.m., line...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>2</td>\n",
       "      <td>Nets get Carter from Raptors</td>\n",
       "      <td>INDIANAPOLIS -- All-Star Vince Carter was trad...</td>\n",
       "      <td>[net, get, carter, raptor]</td>\n",
       "      <td>[all-star, vinc, carter, wa, trade, toronto, r...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class                                              title  \\\n",
       "0           3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1           3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2           3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3           3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4           3  Oil prices soar to all-time record, posing new...   \n",
       "...       ...                                                ...   \n",
       "119995      1  Pakistan's Musharraf Says Won't Quit as Army C...   \n",
       "119996      2                  Renteria signing a top-shelf deal   \n",
       "119997      2                    Saban not going to Dolphins yet   \n",
       "119998      2                                  Today's NFL games   \n",
       "119999      2                       Nets get Carter from Raptors   \n",
       "\n",
       "                                              description  \\\n",
       "0       Reuters - Short-sellers, Wall Street's dwindli...   \n",
       "1       Reuters - Private investment firm Carlyle Grou...   \n",
       "2       Reuters - Soaring crude prices plus worries\\ab...   \n",
       "3       Reuters - Authorities have halted oil export\\f...   \n",
       "4       AFP - Tearaway world oil prices, toppling reco...   \n",
       "...                                                   ...   \n",
       "119995   KARACHI (Reuters) - Pakistani President Perve...   \n",
       "119996  Red Sox general manager Theo Epstein acknowled...   \n",
       "119997  The Miami Dolphins will put their courtship of...   \n",
       "119998  PITTSBURGH at NY GIANTS Time: 1:30 p.m. Line: ...   \n",
       "119999  INDIANAPOLIS -- All-Star Vince Carter was trad...   \n",
       "\n",
       "                                               title_proc  \\\n",
       "0                    [wall, st., bear, claw, back, black]   \n",
       "1              [carlyl, look, toward, commerci, aerospac]   \n",
       "2                   [oil, economi, cloud, stock, outlook]   \n",
       "3       [iraq, halt, oil, export, main, southern, pipe...   \n",
       "4       [oil, price, soar, all-tim, record, pose, new,...   \n",
       "...                                                   ...   \n",
       "119995  [pakistan, musharraf, say, wo, n't, quit, armi...   \n",
       "119996                  [renteria, sign, top-shelf, deal]   \n",
       "119997                          [saban, go, dolphin, yet]   \n",
       "119998                                 [today, nfl, game]   \n",
       "119999                         [net, get, carter, raptor]   \n",
       "\n",
       "                                             descrip_proc        onehot  \n",
       "0       [short-sel, wall, street, dwindling\\band, ultr...  [0, 0, 1, 0]  \n",
       "1       [privat, invest, firm, carlyl, group, \\which, ...  [0, 0, 1, 0]  \n",
       "2       [soar, crude, price, plu, worries\\about, econo...  [0, 0, 1, 0]  \n",
       "3       [author, halt, oil, export\\flow, main, pipelin...  [0, 0, 1, 0]  \n",
       "4       [tearaway, world, oil, price, toppl, record, s...  [0, 0, 1, 0]  \n",
       "...                                                   ...           ...  \n",
       "119995  [pakistani, presid, pervez, musharraf, ha, sai...  [1, 0, 0, 0]  \n",
       "119996  [red, sox, gener, manag, theo, epstein, acknow...  [0, 1, 0, 0]  \n",
       "119997  [miami, dolphin, put, courtship, lsu, coach, n...  [0, 1, 0, 0]  \n",
       "119998  [pittsburgh, ny, giant, time, 1:30, p.m., line...  [0, 1, 0, 0]  \n",
       "119999  [all-star, vinc, carter, wa, trade, toronto, r...  [0, 1, 0, 0]  \n",
       "\n",
       "[120000 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates a dictionary of all words used and their term frequency per class as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "def update_word_dict(row, col):\n",
    "    for word in row[col]:\n",
    "        if not word in word_dict:\n",
    "            word_dict[word] = [0,0,0,0]\n",
    "        word_dict[word][row['class'] - 1] += 1\n",
    "\n",
    "for index, row in training.iterrows():\n",
    "    update_word_dict(row, 'descrip_proc')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sort_top_words method will return the top n words as a dictionary where the value of the word is its place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a very long time.\n",
    "import operator\n",
    "top_words_amount = 2000\n",
    "\n",
    "def sort_top_words(word_dict, top_words_amount, clas):\n",
    "    top_words = {}\n",
    "    for i in range(top_words_amount):\n",
    "        word = max(word_dict, key=lambda word: word_dict[word][clas])\n",
    "        top_words[word] = i\n",
    "        word_dict[word][clas] = 0\n",
    "    return top_words\n",
    "        \n",
    "classes_amount = 4\n",
    "top_words = [sort_top_words(word_dict, top_words_amount, class_num) for class_num in range(classes_amount)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block uses the top_words dictionary as a vocabulary for the tfidf victorizer. The result is the tfidf value of each word in the dictionary corrisponding to its use in each sample article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def identity_tokenizer(row):\n",
    "    return row\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer=identity_tokenizer, \n",
    "    analyzer='word', \n",
    "    preprocessor=identity_tokenizer, \n",
    "    token_pattern=None,\n",
    "    vocabulary=top_words[0]\n",
    ")\n",
    "\n",
    "tfidf.fit(training['descrip_proc'])\n",
    "X_train = tfidf.transform(training['descrip_proc']).toarray()\n",
    "Y_train = np.array(training['class'])\n",
    "Y_train_oh = np.array(training['onehot'].tolist())\n",
    "X_test = tfidf.transform(testing['descrip_proc']).toarray()\n",
    "Y_test = np.array(testing['class'])\n",
    "Y_test_oh = np.array(testing['onehot'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic_regression function is my logistic regression algorithm which I attemt to adhere to the scikit-learn estimator scheme. By implementing the methods get_params, set_params, fit, predict and score, my logistic regression algorithm should be able to be used by GridsearchCV. It uses Mini-Batch Gradient Descent and Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "import random\n",
    "import math\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "\n",
    "class Logistic_regression(BaseEstimator):\n",
    "\n",
    "    def __init__(self, \n",
    "                 epochs = 10, \n",
    "                 batch_samples = 5000, \n",
    "                 learning_rate=0.01, \n",
    "                 gd_type='mini-batch', \n",
    "                 lambda_reg=0.01,\n",
    "                 verbose=True):\n",
    "        self.epochs = epochs\n",
    "        self.batch_samples = batch_samples\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gd_type = gd_type\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        params = {\n",
    "                'epochs': self.epochs,\n",
    "                'batch_samples': self.batch_samples,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'gd_type': self.gd_type,\n",
    "                'lambda_reg': self.lambda_reg\n",
    "            }\n",
    "        return params\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "        \n",
    "    def to_onehot(self, y, classes):\n",
    "        result = np.zeros((y.shape[0], classes.shape[0]))\n",
    "        for i, row in enumerate(result):\n",
    "            row[y[i] - 1] = 1\n",
    "        return result\n",
    "    \n",
    "    def from_onehot(self, y):\n",
    "        return np.array([np.argmax(i) for i in y.T])\n",
    "        \n",
    "    def get_phi(self, x):\n",
    "        return self.w.dot(x)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        phi = self.get_phi(x)\n",
    "        phi_sum = np.zeros(phi.shape[1])\n",
    "        for i in range(phi.shape[0]):\n",
    "            phi_sum += np.exp(phi[i,:])\n",
    "        return phi / phi_sum[None, :]\n",
    "\n",
    "    def gradient(self, p, y, x):\n",
    "        cross_entropy = p - y\n",
    "        return cross_entropy.dot(x.T) + self.lambda_reg * self.w\n",
    "    \n",
    "    # accepts ground truth, y in one hot encoding and predicted vector p and returns float entropy\n",
    "    def cross_entropy(self, y, p):\n",
    "        p = np.array(list(map(lambda i: math.log(i,10), p)))\n",
    "        return np.array(y).dot(p.T) * -1\n",
    "    \n",
    "    def find_loss(self,loss_list):\n",
    "        return np.sum(loss_list) / len(loss_list)\n",
    "    \n",
    "    def mini_batch(self, x, y):\n",
    "        for epoch in range(self.epochs):\n",
    "            gradients = np.zeros(self.w.shape)\n",
    "            xrand = np.random.randint(x.shape[1], size=self.batch_samples)\n",
    "            x = np.array([x[:,i] for i in xrand]).T\n",
    "            y = np.array([y[:,i] for i in xrand]).T\n",
    "            p = self.softmax(x)\n",
    "            dw = self.gradient(p, y, x)\n",
    "            #self.losses.append(self.cross_entropy(y, p))\n",
    "            score = self.score(y, p)\n",
    "            self.w = self.w - self.learning_rate * dw\n",
    "            if self.verbose: print('Epoch %d complete. Score: %1.4f' % (epoch, score))\n",
    "        return self\n",
    "\n",
    "    def stochastic(self, x, y):\n",
    "        for epoch in range(self.epochs):\n",
    "            for _ in range(self.batch_samples):\n",
    "                i = np.random.randint(x.shape[0])\n",
    "                p = self.softmax(x[i])\n",
    "                grad = self.gradient(p, y[i], x[i])\n",
    "                self.losses.append(self.cross_entropy(y[i], p))\n",
    "                self.w = self.w - self.learning_rate * np.add(grad, np.multiply(self.lambda_reg, self.w))\n",
    "            print('Epoch %d complete. Traning loss: %1.4f' % (epoch, self.find_loss(self.losses[-self.batch_samples:])))\n",
    "        return self\n",
    "        \n",
    "    def fit(self, x, y, classes=None):\n",
    "        x = x.T\n",
    "        if classes is None:\n",
    "            y = y.T\n",
    "        else:\n",
    "            y = self.to_onehot(y, classes).T\n",
    "        features = x.shape[0]\n",
    "        classes = y.shape[0]\n",
    "        self.w = np.zeros((classes, features))\n",
    "        if self.gd_type == 'mini-batch':\n",
    "            self.mini_batch(x, y)\n",
    "        if self.gd_type == 'stochastic':\n",
    "            self.stochastic(x, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.softmax(x.T)\n",
    "    \n",
    "    def score(self, y, p):\n",
    "        return accuracy_score(self.from_onehot(y), self.from_onehot(p))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 120000)\n",
      "(4, 120000)\n",
      "Epoch 0 complete. Score: 0.2456\n",
      "Epoch 1 complete. Score: 0.7998\n",
      "Epoch 2 complete. Score: 0.8228\n",
      "Epoch 3 complete. Score: 0.8318\n",
      "Epoch 4 complete. Score: 0.8484\n",
      "Epoch 5 complete. Score: 0.8660\n",
      "Epoch 6 complete. Score: 0.8630\n",
      "Epoch 7 complete. Score: 0.8752\n",
      "Epoch 8 complete. Score: 0.8780\n",
      "Epoch 9 complete. Score: 0.8816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Logistic_regression()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = Logistic_regression(gd_type='mini-batch', learning_rate=0.01)\n",
    "lr.fit(X_train, Y_train, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = svm.SVC(random_state=0)\n",
    "\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "params_to_be_compared = [{\n",
    "        'epochs' : [10, 30, 50],\n",
    "        'batch_samples' : [i * 1000 for i in range(1,9,2)],\n",
    "        'learning_rate' : [1/10**i for i in range(1,6)],\n",
    "        'gd_type' : ['mini-batch'],\n",
    "        'lambda_reg' : [10**i/100 for i in range(1,5)],\n",
    "        'verbose' : False\n",
    "    }]\n",
    "\n",
    "test_p = [{\n",
    "        'epochs': [10,20]\n",
    "    }]\n",
    "\n",
    "print(params_to_be_compared[0].items())\n",
    "lrgs = GridSearchCV(Logistic_regression(), param_grid=test_p, cv=5, scoring='accuracy')\n",
    "lrgs_results = lrgs.fit(X=X_train, y=Y_train, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrgs_results.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete. Score: 0.2478\n",
      "Epoch 1 complete. Score: 0.7822\n",
      "Epoch 2 complete. Score: 0.8040\n",
      "Epoch 3 complete. Score: 0.8122\n",
      "Epoch 4 complete. Score: 0.8330\n",
      "Epoch 5 complete. Score: 0.8422\n",
      "Epoch 6 complete. Score: 0.8604\n",
      "Epoch 7 complete. Score: 0.8572\n",
      "Epoch 8 complete. Score: 0.8636\n",
      "Epoch 9 complete. Score: 0.8666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:687: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mike/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 674, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/mike/.local/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"<ipython-input-44-c41c969617cf>\", line 115, in score\n",
      "    return accuracy_score(self.from_onehot(y), self.from_onehot(p))\n",
      "  File \"/home/mike/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/mike/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py\", line 202, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/home/mike/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py\", line 83, in _check_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/home/mike/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 263, in check_consistent_length\n",
      "    \" samples: %r\" % [int(l) for l in lengths])\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [2000, 4]\n",
      "\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete. Score: 0.2534\n",
      "Epoch 1 complete. Score: 0.8058\n",
      "Epoch 2 complete. Score: 0.8208\n",
      "Epoch 3 complete. Score: 0.8374\n",
      "Epoch 4 complete. Score: 0.8452\n",
      "Epoch 5 complete. Score: 0.8456\n",
      "Epoch 6 complete. Score: 0.8448\n",
      "Epoch 7 complete. Score: 0.8490\n",
      "Epoch 8 complete. Score: 0.8536\n",
      "Epoch 9 complete. Score: 0.8558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:687: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mike/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 674, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/mike/.local/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"<ipython-input-44-c41c969617cf>\", line 115, in score\n",
      "    return accuracy_score(self.from_onehot(y), self.from_onehot(p))\n",
      "  File \"/home/mike/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/mike/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py\", line 202, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/home/mike/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py\", line 83, in _check_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/home/mike/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 263, in check_consistent_length\n",
      "    \" samples: %r\" % [int(l) for l in lengths])\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [2000, 4]\n",
      "\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete. Score: 0.2612\n",
      "Epoch 1 complete. Score: 0.8070\n",
      "Epoch 2 complete. Score: 0.8136\n",
      "Epoch 3 complete. Score: 0.8194\n",
      "Epoch 4 complete. Score: 0.8256\n",
      "Epoch 5 complete. Score: 0.8358\n",
      "Epoch 6 complete. Score: 0.8428\n",
      "Epoch 7 complete. Score: 0.8400\n",
      "Epoch 8 complete. Score: 0.8526\n",
      "Epoch 9 complete. Score: 0.8522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:687: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mike/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 674, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/mike/.local/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"<ipython-input-44-c41c969617cf>\", line 115, in score\n",
      "    return accuracy_score(self.from_onehot(y), self.from_onehot(p))\n",
      "  File \"/home/mike/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/mike/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py\", line 202, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"/home/mike/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py\", line 83, in _check_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/home/mike/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 263, in check_consistent_length\n",
      "    \" samples: %r\" % [int(l) for l in lengths])\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [2000, 4]\n",
      "\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cvresults = cross_validate(Logistic_regression(), X_train, Y_train_oh, cv=3)\n",
    "cvresults['test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along the lines of the logistic regression class, my Multilayer_Perceptron class also adheres to the scikit-learn estimator scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multilayer_Perceptron():\n",
    "    def __init__(self, epochs = 20, neurons = 50, batch_samples = 500, learning_rate=0.01):\n",
    "        self.epochs = epochs\n",
    "        self.neurons = neurons\n",
    "        self.batch_samples = batch_samples\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        params = {\n",
    "                'epochs' : self.epochs,\n",
    "                'neurons' : self.neurons,\n",
    "                'batch_samples' : self.batch_samples,\n",
    "                'learning_rate' : self.learning_rate\n",
    "            }\n",
    "        return params\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def get_z(self, w, x, b):\n",
    "        return w.T.dot(x) + b\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return np.array(list(map(lambda z: 1 / (1 + math.exp(z * -1)), z))).reshape(z.shape)\n",
    "    \n",
    "    def sigmoid_gradient(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        z_sum = 0\n",
    "        for i in z:\n",
    "            z_sum += math.exp(i)\n",
    "        if z_sum == 0:\n",
    "            print(z)\n",
    "        return np.array([math.exp(ze)/z_sum for ze in z]).reshape(z.shape)\n",
    "    \n",
    "        # accepts ground truth, y in one hot encoding and predicted vector p and returns float entropy\n",
    "    def cross_entropy(self, y, p):\n",
    "        p = np.array(list(map(lambda i: math.log(i,10), p)))\n",
    "        return np.array(y).dot(p.T) * -1\n",
    "    \n",
    "    def find_loss(self,loss_list):\n",
    "        return np.sum(loss_list) / len(loss_list)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        features = x.shape[1]\n",
    "        classes = len(y[0])\n",
    "        self.w1 = np.zeros((features, self.neurons))\n",
    "        self.b1 = np.zeros((self.neurons, 1))\n",
    "        self.w2 = np.zeros((self.neurons, classes))\n",
    "        self.b2 = np.zeros((classes, 1))\n",
    "        losses = []\n",
    "        for epoch in range(self.epochs):\n",
    "            dw1 = np.zeros((w1.shape))\n",
    "            db1 = np.zeros((b1.shape))\n",
    "            dw2 = np.zeros((w2.shape))\n",
    "            db2 = np.zeros((b2.shape))\n",
    "            for _ in range(self.batch_samples):\n",
    "                i = np.random.randint(len(x))\n",
    "                \n",
    "                yi = np.array(y[i]).reshape(len(y[i]), 1)\n",
    "                xi = np.array(x[i]).reshape(x[i].shape[0], 1)\n",
    "                \n",
    "                #forward Prop\n",
    "                z1 = self.get_z(w1, xi, b1) #neuronsx1\n",
    "                h1 = self.sigmoid(z1)       #neuronsx1\n",
    "                z2 = self.get_z(w2, h1, b2) #classesx1\n",
    "                h2 = self.softmax(z2)       #classesx1\n",
    "                \n",
    "                #backprop                \n",
    "                dJ_dz2 = h2 - yi            #classsesx1\n",
    "                dw2 += dJ_dz2.dot(h1.T).T    #classesxneurons\n",
    "                db2 += dJ_dz2                #classesx1\n",
    "                dz2_dh1 = w2                #neuronsxclasses\n",
    "                dJ_dh1 = dJ_dz2.T.dot(dz2_dh1.T) #1xneurons\n",
    "                dh1_dz1 = self.sigmoid_gradient(h1) #neuronsx1\n",
    "                dz1_dw1 = xi                #featuresx1\n",
    "                \n",
    "                # Update epoch summations\n",
    "                db1 += dJ_dh1.T * dh1_dz1\n",
    "                dw1 += dz1_dw1.dot(db1.T)\n",
    "                losses.append(self.cross_entropy(y[i], h2))\n",
    "            \n",
    "            # Update weights\n",
    "            w1 = np.subtract(w1, dw1 * self.learning_rate)\n",
    "            b1 = np.subtract(b1, db1 * self.learning_rate)\n",
    "            w2 = np.subtract(w2, dw2 * self.learning_rate)\n",
    "            b2 = np.subtract(b2, db2 * self.learning_rate)\n",
    "            print('Epoch %d complete. Traning loss: %1.4f' % (epoch, self.find_loss(losses[-self.batch_samples:])))\n",
    "            \n",
    "            \n",
    "    def predict(self, x):\n",
    "        xi = np.array(x[i]).reshape(x[i].shape[0], 1)\n",
    "\n",
    "        #forward Prop\n",
    "        z1 = self.get_z(w1, xi, b1) #neuronsx1\n",
    "        h1 = self.sigmoid(z1)       #neuronsx1\n",
    "        z2 = self.get_z(w2, h1, b2) #classesx1\n",
    "        h2 = self.softmax(z2)       #classesx1\n",
    "        return h2\n",
    "\n",
    "    def score(self, x, y):\n",
    "        p = self.predict(x)\n",
    "        return self.cross_entropy(y, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete. Traning loss: 0.6021\n",
      "Epoch 1 complete. Traning loss: 1.0276\n",
      "Epoch 2 complete. Traning loss: 5.6948\n",
      "Epoch 3 complete. Traning loss: 1.7459\n",
      "Epoch 4 complete. Traning loss: 0.7804\n",
      "Epoch 5 complete. Traning loss: 0.6735\n",
      "Epoch 6 complete. Traning loss: 0.6070\n",
      "Epoch 7 complete. Traning loss: 0.5977\n",
      "Epoch 8 complete. Traning loss: 0.6035\n",
      "Epoch 9 complete. Traning loss: 0.6034\n",
      "Epoch 10 complete. Traning loss: 0.6053\n",
      "Epoch 11 complete. Traning loss: 0.6078\n",
      "Epoch 12 complete. Traning loss: 0.6032\n",
      "Epoch 13 complete. Traning loss: 0.6034\n",
      "Epoch 14 complete. Traning loss: 0.6102\n",
      "Epoch 15 complete. Traning loss: 0.6086\n",
      "Epoch 16 complete. Traning loss: 0.6000\n",
      "Epoch 17 complete. Traning loss: 0.6051\n",
      "Epoch 18 complete. Traning loss: 0.6119\n",
      "Epoch 19 complete. Traning loss: 0.6062\n"
     ]
    }
   ],
   "source": [
    "mlp = Multilayer_Perceptron()\n",
    "mlp.fit(X_train, Y_train) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
