{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mike/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mike/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Get data from file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update([',','\\'','.','\\\"','...','`','#','$','%','&','*',';',':','/b','u','gt','lt','//','\\'s','\\'\\'','-','reuter'])\n",
    "classes = 4\n",
    "training_filename = 'ag_news_csv/train.csv'\n",
    "testing_filename = 'ag_news_csv/test.csv'\n",
    "col_names = ['class','title','description']\n",
    "training = pd.read_csv(training_filename, names=col_names)\n",
    "testing = pd.read_csv(testing_filename, names=col_names)\n",
    "\n",
    "tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "rgx_list = ['(\\w+[A-Z]+.*\\-+\\s)','(\\({1}\\w+[\\.*\\s*\\w*]*\\){1})']\n",
    "\n",
    "def token_stem_lem(text):\n",
    "    # remove location of article before first - , and any source denoted by parenthesis\n",
    "    new_text = text\n",
    "    for rgx_match in rgx_list:\n",
    "        new_text = re.sub(rgx_match, '', new_text)\n",
    "    words_stemmed_lemmed = []\n",
    "    for word in tokenizer.tokenize(new_text):\n",
    "        new_word = stemmer.stem(lemmer.lemmatize(word.lower()))\n",
    "        if new_word not in stop_words :\n",
    "            words_stemmed_lemmed.append(new_word)\n",
    "    return words_stemmed_lemmed\n",
    "\n",
    "def to_onehot(y, class_rng):\n",
    "    res = []\n",
    "    for i in class_rng:\n",
    "        if y is i:\n",
    "            res.append(1)\n",
    "        else:\n",
    "            res.append(0)\n",
    "    return res\n",
    "\n",
    "testing['title_proc'] = testing['title'].apply(lambda title: token_stem_lem(title))\n",
    "testing['descrip_proc'] = testing['description'].apply(lambda desc: token_stem_lem(desc))\n",
    "testing['onehot'] = testing['class'].apply(lambda y: to_onehot(y, range(1, classes + 1)))\n",
    "training['title_proc'] = training['title'].apply(lambda title: token_stem_lem(title))\n",
    "training['descrip_proc'] = training['description'].apply(lambda desc: token_stem_lem(desc))\n",
    "training['onehot'] = training['class'].apply(lambda y: to_onehot(y, range(1, classes + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>title_proc</th>\n",
       "      <th>descrip_proc</th>\n",
       "      <th>onehot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "      <td>[wall, st., bear, claw, back, black]</td>\n",
       "      <td>[short-sel, wall, street, dwindling\\band, ultr...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>[carlyl, look, toward, commerci, aerospac]</td>\n",
       "      <td>[privat, invest, firm, carlyl, group, \\which, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>[oil, economi, cloud, stock, outlook]</td>\n",
       "      <td>[soar, crude, price, plu, worries\\about, econo...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "      <td>[iraq, halt, oil, export, main, southern, pipe...</td>\n",
       "      <td>[author, halt, oil, export\\flow, main, pipelin...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "      <td>[oil, price, soar, all-tim, record, pose, new,...</td>\n",
       "      <td>[tearaway, world, oil, price, toppl, record, s...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1</td>\n",
       "      <td>Pakistan's Musharraf Says Won't Quit as Army C...</td>\n",
       "      <td>KARACHI (Reuters) - Pakistani President Perve...</td>\n",
       "      <td>[pakistan, musharraf, say, wo, n't, quit, armi...</td>\n",
       "      <td>[pakistani, presid, pervez, musharraf, ha, sai...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>2</td>\n",
       "      <td>Renteria signing a top-shelf deal</td>\n",
       "      <td>Red Sox general manager Theo Epstein acknowled...</td>\n",
       "      <td>[renteria, sign, top-shelf, deal]</td>\n",
       "      <td>[red, sox, gener, manag, theo, epstein, acknow...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>2</td>\n",
       "      <td>Saban not going to Dolphins yet</td>\n",
       "      <td>The Miami Dolphins will put their courtship of...</td>\n",
       "      <td>[saban, go, dolphin, yet]</td>\n",
       "      <td>[miami, dolphin, put, courtship, lsu, coach, n...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>2</td>\n",
       "      <td>Today's NFL games</td>\n",
       "      <td>PITTSBURGH at NY GIANTS Time: 1:30 p.m. Line: ...</td>\n",
       "      <td>[today, nfl, game]</td>\n",
       "      <td>[pittsburgh, ny, giant, time, 1:30, p.m., line...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>2</td>\n",
       "      <td>Nets get Carter from Raptors</td>\n",
       "      <td>INDIANAPOLIS -- All-Star Vince Carter was trad...</td>\n",
       "      <td>[net, get, carter, raptor]</td>\n",
       "      <td>[all-star, vinc, carter, wa, trade, toronto, r...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class                                              title  \\\n",
       "0           3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1           3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2           3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3           3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4           3  Oil prices soar to all-time record, posing new...   \n",
       "...       ...                                                ...   \n",
       "119995      1  Pakistan's Musharraf Says Won't Quit as Army C...   \n",
       "119996      2                  Renteria signing a top-shelf deal   \n",
       "119997      2                    Saban not going to Dolphins yet   \n",
       "119998      2                                  Today's NFL games   \n",
       "119999      2                       Nets get Carter from Raptors   \n",
       "\n",
       "                                              description  \\\n",
       "0       Reuters - Short-sellers, Wall Street's dwindli...   \n",
       "1       Reuters - Private investment firm Carlyle Grou...   \n",
       "2       Reuters - Soaring crude prices plus worries\\ab...   \n",
       "3       Reuters - Authorities have halted oil export\\f...   \n",
       "4       AFP - Tearaway world oil prices, toppling reco...   \n",
       "...                                                   ...   \n",
       "119995   KARACHI (Reuters) - Pakistani President Perve...   \n",
       "119996  Red Sox general manager Theo Epstein acknowled...   \n",
       "119997  The Miami Dolphins will put their courtship of...   \n",
       "119998  PITTSBURGH at NY GIANTS Time: 1:30 p.m. Line: ...   \n",
       "119999  INDIANAPOLIS -- All-Star Vince Carter was trad...   \n",
       "\n",
       "                                               title_proc  \\\n",
       "0                    [wall, st., bear, claw, back, black]   \n",
       "1              [carlyl, look, toward, commerci, aerospac]   \n",
       "2                   [oil, economi, cloud, stock, outlook]   \n",
       "3       [iraq, halt, oil, export, main, southern, pipe...   \n",
       "4       [oil, price, soar, all-tim, record, pose, new,...   \n",
       "...                                                   ...   \n",
       "119995  [pakistan, musharraf, say, wo, n't, quit, armi...   \n",
       "119996                  [renteria, sign, top-shelf, deal]   \n",
       "119997                          [saban, go, dolphin, yet]   \n",
       "119998                                 [today, nfl, game]   \n",
       "119999                         [net, get, carter, raptor]   \n",
       "\n",
       "                                             descrip_proc        onehot  \n",
       "0       [short-sel, wall, street, dwindling\\band, ultr...  [0, 0, 1, 0]  \n",
       "1       [privat, invest, firm, carlyl, group, \\which, ...  [0, 0, 1, 0]  \n",
       "2       [soar, crude, price, plu, worries\\about, econo...  [0, 0, 1, 0]  \n",
       "3       [author, halt, oil, export\\flow, main, pipelin...  [0, 0, 1, 0]  \n",
       "4       [tearaway, world, oil, price, toppl, record, s...  [0, 0, 1, 0]  \n",
       "...                                                   ...           ...  \n",
       "119995  [pakistani, presid, pervez, musharraf, ha, sai...  [1, 0, 0, 0]  \n",
       "119996  [red, sox, gener, manag, theo, epstein, acknow...  [0, 1, 0, 0]  \n",
       "119997  [miami, dolphin, put, courtship, lsu, coach, n...  [0, 1, 0, 0]  \n",
       "119998  [pittsburgh, ny, giant, time, 1:30, p.m., line...  [0, 1, 0, 0]  \n",
       "119999  [all-star, vinc, carter, wa, trade, toronto, r...  [0, 1, 0, 0]  \n",
       "\n",
       "[120000 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "def update_word_dict(row, col):\n",
    "    for word in row[col]:\n",
    "        if not word in word_dict:\n",
    "            word_dict[word] = [0,0,0,0]\n",
    "        word_dict[word][row['class'] - 1] += 1\n",
    "\n",
    "for index, row in training.iterrows():\n",
    "    update_word_dict(row, 'descrip_proc')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a very long time.\n",
    "import operator\n",
    "top_words_amount = 2000\n",
    "\n",
    "def reduce_word_dict(wd, key):\n",
    "    r = dict(wd)\n",
    "    del r[key]\n",
    "    return r\n",
    "\n",
    "def sort_top_words(word_dict, top_words_amount, clas):\n",
    "    top_words = {}\n",
    "    for i in range(top_words_amount):\n",
    "        word = max(word_dict, key=lambda word: word_dict[word][clas])\n",
    "        top_words[word] = i\n",
    "        word_dict[word][clas] = 0\n",
    "    return top_words\n",
    "        \n",
    "classes_amount = 4\n",
    "top_words = [sort_top_words(word_dict, top_words_amount, class_num) for class_num in range(classes_amount)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def identity_tokenizer(row):\n",
    "    return row\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer=identity_tokenizer, \n",
    "    analyzer='word', \n",
    "    preprocessor=identity_tokenizer, \n",
    "    token_pattern=None,\n",
    "    vocabulary=top_words[0]\n",
    ")\n",
    "tfidf.fit(training['descrip_proc'])\n",
    "X_train = tfidf.transform(training['descrip_proc']).toarray()\n",
    "Y_train = training['onehot']\n",
    "X_test = tfidf.transform(testing['descrip_proc']).toarray()\n",
    "Y_test = testing['onehot']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "import random\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "class Logistic_regression():\n",
    "\n",
    "    def __init__(self, classes, epochs = 10, validation_section = 0, batch_samples = 5000, learning_rate=0.01, gd_type='mini-batch', lambda_reg=0.01):\n",
    "        self.classes = classes\n",
    "        self.epochs = epochs\n",
    "        self.validation_section = validation_section\n",
    "        self.batch_samples = batch_samples\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gd_type = gd_type\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.losses = []\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        params = [{\n",
    "                'batch_samples': self.batch_samples,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'gd_type': self.gd_type,\n",
    "                'lambda_reg': self.lambda_reg\n",
    "            }]\n",
    "        return params\n",
    "        \n",
    "    def get_phi(self, x):\n",
    "        return self.w.dot(x.T)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        phi_sum = 0\n",
    "        phi = self.get_phi(x)\n",
    "        for i in phi:\n",
    "            phi_sum += math.exp(i)\n",
    "        return [math.exp(phie)/phi_sum for phie in phi]\n",
    "\n",
    "    def gradient(self, p, y, x):\n",
    "        cross_entropy = np.subtract(p,y)\n",
    "        x = np.array(x)\n",
    "        return np.asarray([i * x for i in cross_entropy])\n",
    "    \n",
    "    # accepts ground truth, y in one hot encoding and predicted vector p and returns float entropy\n",
    "    def cross_entropy(self, y, p):\n",
    "        p = np.array(list(map(lambda i: math.log(i,10), p)))\n",
    "        return np.array(y).dot(p.T) * -1\n",
    "    \n",
    "    def find_loss(self,loss_list):\n",
    "        return np.sum(loss_list) / len(loss_list)\n",
    "    \n",
    "    def mini_batch(self, x, y):\n",
    "        for epoch in range(self.epochs):\n",
    "            gradients = np.zeros(self.w.shape)\n",
    "            for _ in range(self.batch_samples):\n",
    "                i = np.random.randint(len(x))\n",
    "                p = self.softmax(x[i])\n",
    "                gradients = np.add(gradients, self.gradient(p, y[i], x[i]))\n",
    "                self.losses.append(self.cross_entropy(y[i], p))\n",
    "            reg = np.multiply(self.lambda_reg, self.w)\n",
    "            self.w = self.w - self.learning_rate * np.add(gradients, reg)\n",
    "            print('Epoch %d complete. Traning loss: %1.4f' % (epoch, self.find_loss(self.losses[-self.batch_samples:])))\n",
    "\n",
    "    def stochastic(self, x, y):\n",
    "        for epoch in range(self.epochs):\n",
    "            for _ in range(self.batch_samples):\n",
    "                i = np.random.randint(len(x))\n",
    "                p = self.softmax(x[i])\n",
    "                grad = self.gradient(p, y[i], x[i])\n",
    "                self.losses.append(self.cross_entropy(y[i], p))\n",
    "                self.w = self.w - self.learning_rate * np.add(grad, np.multiply(self.lambda_reg, self.w))\n",
    "            print('Epoch %d complete. Traning loss: %1.4f' % (epoch, self.find_loss(self.losses[-self.batch_samples:])))\n",
    "\n",
    "        \n",
    "    def fit(self, x,y):\n",
    "        features = x.shape[1]\n",
    "        self.w = np.zeros((classes, features))\n",
    "        val_range_min = 0\n",
    "        val_range_max = 0\n",
    "        if self.gd_type == 'mini-batch':\n",
    "            self.mini_batch(x, y)\n",
    "        if self.gd_type == 'stochastic':\n",
    "            self.stochastic(x, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.softmax(x)\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        p = self.predict(x)\n",
    "        return self.cross_entropy(y, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete. Traning loss: 0.6021\n",
      "Epoch 1 complete. Traning loss: 0.5613\n",
      "Epoch 2 complete. Traning loss: 0.5259\n",
      "Epoch 3 complete. Traning loss: 0.4961\n",
      "Epoch 4 complete. Traning loss: 0.4698\n",
      "Epoch 5 complete. Traning loss: 0.4507\n",
      "Epoch 6 complete. Traning loss: 0.4338\n",
      "Epoch 7 complete. Traning loss: 0.4142\n",
      "Epoch 8 complete. Traning loss: 0.4031\n",
      "Epoch 9 complete. Traning loss: 0.3887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Logistic_regression at 0x7fa4c3de6470>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = Logistic_regression(classes, gd_type='mini-batch', learning_rate=0.01)\n",
    "lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_be_compared = [{\n",
    "        'batch_samples':[i * 10000 for i in range(0,9,2)],\n",
    "        'learning_rate':[1/10**i for i in range(1,6)],\n",
    "        'gd_type':['mini-batch'],\n",
    "        'lambda_reg':[10**i/100 for i in range(1,5)]\n",
    "    }]\n",
    "lrgs = GridSearchCV(Logistic_regression(4), param_grid=params_to_be_compared, scoring='accuracy')\n",
    "lrgs_results = lrgs.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_mini_batch():\n",
    "    for epoch in range(self.epochs):\n",
    "        epoch_range_min = len(x)/self.epochs * epoch\n",
    "        epoch_range_max = len(x)/self.epochs * (epoch + 1)\n",
    "        if epoch == self.validation_section:\n",
    "            val_range_min = epoch_range_min\n",
    "            val_range_max = epoch_range_max\n",
    "        else:\n",
    "            for _ in range(self.batch_samples):\n",
    "                i = random.randint(epoch_range_min, epoch_range_max - 1)\n",
    "                p = self.softmax(x[i])\n",
    "                grad = self.gradient(p, y[i], x[i])\n",
    "                self.losses.append(self.cross_entropy(y[i], p))\n",
    "                self.w = self.w - self.learning_rate * np.add(grad, np.multiply(self.lambda_reg, self.w))\n",
    "            print('Epoch %d complete. Traning loss: %1.4f' % (epoch, self.find_loss(self.losses[-self.batch_samples:])))\n",
    "    # validation batch\n",
    "    for _ in range(self.batch_samples):\n",
    "        i = random.randint(epoch_range_min, epoch_range_max - 1)\n",
    "        p = self.softmax(x[i])\n",
    "        grad = self.gradient(p, y[i], x[i])\n",
    "        self.losses.append(self.cross_entropy(y[i], p))\n",
    "    print('Epoch %d complete. Traning loss: %1.4f' % (epoch, self.find_loss(self.losses[-self.batch_samples:])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multilayer_Perceptron():\n",
    "    def __init__(self, epochs = 10, neurons = 50, batch_samples = 5000, learning_rate=0.01):\n",
    "        self.epochs = epochs\n",
    "        self.neurons = neurons\n",
    "        self.batch_samples = batch_samples\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        params = [{\n",
    "                'epochs' : self.epochs,\n",
    "                'neurons' : self.neurons,\n",
    "                'batch_samples' : self.batch_samples,\n",
    "                'learning_rate' : self.learning_rate\n",
    "            }]\n",
    "        return params\n",
    "    \n",
    "    def get_z(self, w, x):\n",
    "        return w.dot(x.T)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return np.array(list(map(lambda z: 1 / (1 + math.exp(z * -1)), z)))\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        z_sum = 0\n",
    "        for i in z:\n",
    "            z_sum += math.exp(i)\n",
    "        return [math.exp(ze)/z_sum for ze in z]\n",
    "    \n",
    "        # accepts ground truth, y in one hot encoding and predicted vector p and returns float entropy\n",
    "    def cross_entropy(self, y, p):\n",
    "        p = np.array(list(map(lambda i: math.log(i,10), p)))\n",
    "        return np.array(y).dot(p.T) * -1\n",
    "    \n",
    "    def find_loss(self,loss_list):\n",
    "        return np.sum(loss_list) / len(loss_list)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        w1 = np.zeros((self.neurons, x.shape[1]))\n",
    "        w2 = np.zeros((len(y[0]), self.neurons))\n",
    "        w1g = np.zeros((w1.shape))\n",
    "        w2g = np.zeros((w2.shape))\n",
    "        losses = []\n",
    "        for epoch in range(self.epochs):\n",
    "            for _ in range(self.batch_samples):\n",
    "                i = np.random.randint(len(x))\n",
    "                #forward Prop\n",
    "                z1 = self.get_z(w1, x[i])\n",
    "                h1 = self.sigmoid(z1)\n",
    "                z2 = self.get_z(w2, h1)\n",
    "                h2 = self.softmax(z2)\n",
    "                #backprop\n",
    "                yi = np.array(y[i]).reshape(len(y[i]),1)\n",
    "                w2_grad = yi.dot(h1.reshape(1, h1.shape[0]))\n",
    "                w1_grad_sch = yi.T.dot(w2).dot(h1)\n",
    "                w1_grad = np.array([h1e * w1_grad_sch * -1 for h1e in h1]).reshape(h1.shape[0],1).dot(x[i].reshape(x[i].shape[0],1).T)\n",
    "                w1g = np.add(w1g, w1_grad)\n",
    "                w2g = np.add(w2g, w2_grad)\n",
    "                losses.append(self.cross_entropy(y[i], h2))\n",
    "            w1 = np.subtract(w1, w1g * self.learning_rate)\n",
    "            w2 = np.subtract(w2, w2g * self.learning_rate)\n",
    "            print('Epoch %d complete. Traning loss: %1.4f' % (epoch, self.find_loss(losses[-self.batch_samples:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2000)\n",
      "Epoch 0 complete. Traning loss: 0.6021\n",
      "(50, 2000)\n",
      "Epoch 1 complete. Traning loss: 1.2039\n",
      "(50, 2000)\n",
      "Epoch 2 complete. Traning loss: 0.6021\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-df1fdc0dd4b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultilayer_Perceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-214-1f5418aec1c5>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mz2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0;31m#backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0myi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-214-1f5418aec1c5>\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mz_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mze\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mz_sum\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mze\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# accepts ground truth, y in one hot encoding and predicted vector p and returns float entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-214-1f5418aec1c5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mz_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mze\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mz_sum\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mze\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# accepts ground truth, y in one hot encoding and predicted vector p and returns float entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "mlp = Multilayer_Perceptron()\n",
    "mlp.fit(X_train, Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_train[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
