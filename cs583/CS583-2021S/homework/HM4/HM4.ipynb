{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home 4: Build a CNN for image recognition.\n",
    "\n",
    "### Name: Michael Eng\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read, complete, and run the code.\n",
    "\n",
    "2. **Make substantial improvements** to maximize the accurcy.\n",
    "    \n",
    "3. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "    \n",
    "    * Missing **the output after execution** will not be graded.\n",
    "    \n",
    "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo. (If you submit the file to Google Drive or Dropbox, you must make the file \"open-access\". The delay caused by \"deny of access\" may result in late penalty.)\n",
    "\n",
    "4. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583-2020S/blob/master/homework/HM4/HM4.html\n",
    "\n",
    "\n",
    "## Requirements:\n",
    "\n",
    "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
    "\n",
    "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
    "\n",
    "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
    "\n",
    "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
    "\n",
    "\n",
    "## Google Colab\n",
    "\n",
    "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
    "\n",
    "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
    "\n",
    "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
    "\n",
    "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (50000, 32, 32, 3)\n",
      "shape of y_train: (50000, 1)\n",
      "shape of x_test: (10000, 32, 32, 3)\n",
      "shape of y_test: (10000, 1)\n",
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (50000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "[6]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y, num_class=10):\n",
    "    res = numpy.zeros((y.shape[0], num_class))\n",
    "    for i,val in enumerate(y):\n",
    "        res[i, val] = 1\n",
    "    return res\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets:\n",
    "* a training set containing 40K samples\n",
    "* a validation set containing 10K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\n",
      "Shape of y_tr: (40000, 10)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = numpy.random.permutation(50000)\n",
    "train_indices = rand_indices[0:40000]\n",
    "valid_indices = rand_indices[40000:50000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters\n",
    "\n",
    "1. Build a convolutional neural network model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "3. Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark: \n",
    "\n",
    "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
    "* Add more layers.\n",
    "* Use regularizations, e.g., dropout.\n",
    "* Use batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense, Activation, Dropout, ZeroPadding2D, Add, LeakyReLU\n",
    "from tensorflow.keras import utils, Input, initializers, optimizers\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, RandomContrast, RandomWidth, RandomZoom\n",
    "\n",
    "learning_rate = 1E-5 # to be tuned!\n",
    "\n",
    "\n",
    "def bn_relu(input):\n",
    "    bn = BatchNormalization(axis=3)(input)\n",
    "    return Activation('relu')(bn)\n",
    "\n",
    "def residual_block(x, downsample: bool, filters: int, kernel_size: int = 3):\n",
    "    y = Conv2D(kernel_size=kernel_size, strides=(1 if not downsample else 2), filters=filters, padding='same')(x)\n",
    "    y = bn_relu(y)\n",
    "    y = Conv2D(kernel_size=kernel_size, strides=1, filters=filters, padding='same')(y)\n",
    "    if downsample:\n",
    "        x = Conv2D(kernel_size=kernel_size, strides=2, filters=filters, padding='same')(x)\n",
    "    out = Add()([x,y])\n",
    "    out = bn_relu(out)\n",
    "    return out\n",
    "\n",
    "def build_resnet(dropout = 0.0, num_blocks_list = [2,5,5,2]):\n",
    "    num_filters = 64\n",
    "    X_input = Input(shape=(32,32,3))\n",
    "    X = ZeroPadding2D(padding=(3,3), data_format=None)(X_input)\n",
    "    X = Conv2D(filters = num_filters, kernel_size=(7,7), strides=(1,1), name = 'conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3,3), strides=(2,2))(X)\n",
    "    # Stage 2 for ResNet\n",
    "\n",
    "    for i,blocks in enumerate(num_blocks_list):\n",
    "        for j in range(blocks):\n",
    "            X = residual_block(X, downsample=(j==0 and i!=0), filters=num_filters)\n",
    "        if num_filters < 256:\n",
    "            num_filters *= 2\n",
    "\n",
    "    x = AveragePooling2D(pool_size=(4,4), padding='same')(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dropout(dropout)(X)\n",
    "    X = Dense(num_filters, activation='relu', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = Dense(NUM_CLASSES, activation='softmax', name='fc' + str(NUM_CLASSES), kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    name = 'ResNetDropOut' + str(int(dropout * 100))\n",
    "    model = Model(inputs = X_input, outputs = X, name=name)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune learning rate\n",
    "The first learning rate attempted is 10^-5, which is then increased to 10^-4 and 10^-3. The best results were with a learning rate of 10^-4 so I tried a learning rate in the middle of 5 * 10^-5. \n",
    "\n",
    "The result was as follows:\n",
    "\n",
    "10^-5\n",
    "training acc: 0.8596\n",
    "val acc: 0.4551\n",
    "\n",
    "5 * 10^-5\n",
    "acc: 0.9391\n",
    "val acc: 0.5844\n",
    "\n",
    "10^-4\n",
    "training acc: 0.9458\n",
    "val acc: 0.6327\n",
    "\n",
    "10^-3\n",
    "training acc: 0.4317\n",
    "val acc: 0.3320\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 386s 307ms/step - loss: 2.2652 - acc: 0.2045 - val_loss: 1.8775 - val_acc: 0.3286\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 381s 305ms/step - loss: 1.7055 - acc: 0.3891 - val_loss: 1.7104 - val_acc: 0.3907\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 381s 305ms/step - loss: 1.4359 - acc: 0.4840 - val_loss: 1.6130 - val_acc: 0.4275\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 384s 307ms/step - loss: 1.2465 - acc: 0.5562 - val_loss: 1.5921 - val_acc: 0.4432\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 397s 317ms/step - loss: 1.0573 - acc: 0.6290 - val_loss: 1.6069 - val_acc: 0.4393\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 406s 325ms/step - loss: 0.9151 - acc: 0.6850 - val_loss: 1.6487 - val_acc: 0.4608\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 382s 306ms/step - loss: 0.7798 - acc: 0.7374 - val_loss: 1.6957 - val_acc: 0.4540\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 390s 312ms/step - loss: 0.6486 - acc: 0.7849 - val_loss: 1.7254 - val_acc: 0.4594\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 382s 306ms/step - loss: 0.5294 - acc: 0.8262 - val_loss: 1.8288 - val_acc: 0.4629\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 391s 313ms/step - loss: 0.4356 - acc: 0.8596 - val_loss: 1.9316 - val_acc: 0.4551\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(x_tr, y_tr, batch_size=32, epochs=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate 10^-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNetDropOut0\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 38, 38, 3)    0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 32, 32, 64)   9472        zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 32, 32, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 32, 32, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 15, 15, 64)   0           activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 15, 15, 64)   36928       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 15, 15, 64)   256         conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 15, 15, 64)   0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 15, 15, 64)   36928       activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_56 (Add)                    (None, 15, 15, 64)   0           max_pooling2d_4[0][0]            \n",
      "                                                                 conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 15, 15, 64)   256         add_56[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 15, 15, 64)   0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 15, 15, 64)   36928       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 15, 15, 64)   256         conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 15, 15, 64)   0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 15, 15, 64)   36928       activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_57 (Add)                    (None, 15, 15, 64)   0           activation_118[0][0]             \n",
      "                                                                 conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 15, 15, 64)   256         add_57[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 15, 15, 64)   0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 8, 8, 128)    73856       activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 8, 8, 128)    512         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 8, 8, 128)    0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 8, 8, 128)    73856       activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 8, 8, 128)    147584      activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_58 (Add)                    (None, 8, 8, 128)    0           conv2d_130[0][0]                 \n",
      "                                                                 conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 8, 8, 128)    512         add_58[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 8, 8, 128)    0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 8, 8, 128)    147584      activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 8, 8, 128)    512         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 8, 8, 128)    0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 8, 8, 128)    147584      activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_59 (Add)                    (None, 8, 8, 128)    0           activation_122[0][0]             \n",
      "                                                                 conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 8, 8, 128)    512         add_59[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 8, 8, 128)    0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 8, 8, 128)    147584      activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 8, 8, 128)    512         conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 8, 8, 128)    0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 8, 8, 128)    147584      activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_60 (Add)                    (None, 8, 8, 128)    0           activation_124[0][0]             \n",
      "                                                                 conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 8, 8, 128)    512         add_60[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 8, 8, 128)    0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 8, 8, 128)    147584      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 8, 8, 128)    512         conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 8, 8, 128)    0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 8, 8, 128)    147584      activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_61 (Add)                    (None, 8, 8, 128)    0           activation_126[0][0]             \n",
      "                                                                 conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 8, 8, 128)    512         add_61[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 8, 8, 128)    0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 8, 8, 128)    147584      activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 8, 8, 128)    512         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 8, 8, 128)    0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 8, 8, 128)    147584      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_62 (Add)                    (None, 8, 8, 128)    0           activation_128[0][0]             \n",
      "                                                                 conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 8, 8, 128)    512         add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 8, 8, 128)    0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 4, 4, 256)    295168      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 4, 4, 256)    1024        conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 4, 4, 256)    0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 4, 4, 256)    295168      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 4, 4, 256)    590080      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_63 (Add)                    (None, 4, 4, 256)    0           conv2d_141[0][0]                 \n",
      "                                                                 conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 4, 4, 256)    1024        add_63[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 4, 4, 256)    0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 4, 4, 256)    590080      activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 4, 4, 256)    1024        conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 4, 4, 256)    0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 4, 4, 256)    590080      activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_64 (Add)                    (None, 4, 4, 256)    0           activation_132[0][0]             \n",
      "                                                                 conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 4, 4, 256)    1024        add_64[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 4, 4, 256)    0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 4, 4, 256)    590080      activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 4, 4, 256)    1024        conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 4, 4, 256)    0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 4, 4, 256)    590080      activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_65 (Add)                    (None, 4, 4, 256)    0           activation_134[0][0]             \n",
      "                                                                 conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 4, 4, 256)    1024        add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 4, 4, 256)    0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 4, 4, 256)    590080      activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 4, 4, 256)    1024        conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 4, 4, 256)    0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 4, 4, 256)    590080      activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_66 (Add)                    (None, 4, 4, 256)    0           activation_136[0][0]             \n",
      "                                                                 conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 4, 4, 256)    1024        add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 4, 4, 256)    0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 4, 4, 256)    590080      activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 4, 4, 256)    1024        conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 4, 4, 256)    0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 4, 4, 256)    590080      activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_67 (Add)                    (None, 4, 4, 256)    0           activation_138[0][0]             \n",
      "                                                                 conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 4, 4, 256)    1024        add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 4, 4, 256)    0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 2, 2, 512)    1180160     activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 2, 2, 512)    2048        conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 2, 2, 512)    0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 2, 2, 512)    1180160     activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 2, 2, 512)    2359808     activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_68 (Add)                    (None, 2, 2, 512)    0           conv2d_152[0][0]                 \n",
      "                                                                 conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 2, 2, 512)    2048        add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 2, 2, 512)    0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 2, 2, 512)    2359808     activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 2, 2, 512)    2048        conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 2, 2, 512)    0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 2, 2, 512)    2359808     activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_69 (Add)                    (None, 2, 2, 512)    0           activation_142[0][0]             \n",
      "                                                                 conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 2, 2, 512)    2048        add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 2, 2, 512)    0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 2048)         0           activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 2048)         0           flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "fc10 (Dense)                    (None, 10)           20490       dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,019,274\n",
      "Trainable params: 17,006,858\n",
      "Non-trainable params: 12,416\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = build_resnet()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 418s 332ms/step - loss: 1.9542 - acc: 0.3360 - val_loss: 1.3977 - val_acc: 0.5039\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 403s 322ms/step - loss: 1.1500 - acc: 0.5894 - val_loss: 1.3436 - val_acc: 0.5520\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 402s 322ms/step - loss: 0.8690 - acc: 0.6938 - val_loss: 1.4567 - val_acc: 0.5231\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 412s 330ms/step - loss: 0.6522 - acc: 0.7714 - val_loss: 1.3527 - val_acc: 0.5910\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 411s 329ms/step - loss: 0.4702 - acc: 0.8390 - val_loss: 1.3161 - val_acc: 0.6123\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 427s 342ms/step - loss: 0.3474 - acc: 0.8813 - val_loss: 1.3437 - val_acc: 0.6467\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 427s 342ms/step - loss: 0.2664 - acc: 0.9107 - val_loss: 1.1913 - val_acc: 0.6683\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 412s 330ms/step - loss: 0.2164 - acc: 0.9266 - val_loss: 1.3502 - val_acc: 0.6726\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 406s 325ms/step - loss: 0.1870 - acc: 0.9386 - val_loss: 1.5184 - val_acc: 0.6515\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 398s 318ms/step - loss: 0.1639 - acc: 0.9458 - val_loss: 1.7320 - val_acc: 0.6327\n"
     ]
    }
   ],
   "source": [
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate * 10),\n",
    "              metrics=['acc'])\n",
    "history2 = model2.fit(x_tr, y_tr, batch_size=32, epochs=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate 10^-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNetDropOut0\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 38, 38, 3)    0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 32, 32, 64)   9472        zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 32, 32, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 32, 32, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 15, 15, 64)   0           activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 15, 15, 64)   36928       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 15, 15, 64)   256         conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 15, 15, 64)   0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 15, 15, 64)   36928       activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_70 (Add)                    (None, 15, 15, 64)   0           max_pooling2d_5[0][0]            \n",
      "                                                                 conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 15, 15, 64)   256         add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 15, 15, 64)   0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 15, 15, 64)   36928       activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 15, 15, 64)   256         conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 15, 15, 64)   0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 15, 15, 64)   36928       activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_71 (Add)                    (None, 15, 15, 64)   0           activation_147[0][0]             \n",
      "                                                                 conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 15, 15, 64)   256         add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 15, 15, 64)   0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 8, 8, 128)    73856       activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 8, 8, 128)    512         conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 8, 8, 128)    0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 8, 8, 128)    73856       activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 8, 8, 128)    147584      activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_72 (Add)                    (None, 8, 8, 128)    0           conv2d_161[0][0]                 \n",
      "                                                                 conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 8, 8, 128)    512         add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 8, 8, 128)    0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 8, 8, 128)    147584      activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 8, 8, 128)    512         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 8, 8, 128)    0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 8, 8, 128)    147584      activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_73 (Add)                    (None, 8, 8, 128)    0           activation_151[0][0]             \n",
      "                                                                 conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 8, 8, 128)    512         add_73[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 8, 8, 128)    0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 8, 8, 128)    147584      activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 8, 8, 128)    512         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 8, 8, 128)    0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 8, 8, 128)    147584      activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_74 (Add)                    (None, 8, 8, 128)    0           activation_153[0][0]             \n",
      "                                                                 conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 8, 8, 128)    512         add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 8, 8, 128)    0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 8, 8, 128)    147584      activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 8, 8, 128)    512         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 8, 8, 128)    0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 8, 8, 128)    147584      activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_75 (Add)                    (None, 8, 8, 128)    0           activation_155[0][0]             \n",
      "                                                                 conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 8, 8, 128)    512         add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 8, 8, 128)    0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 8, 8, 128)    147584      activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 8, 8, 128)    512         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 8, 8, 128)    0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 8, 8, 128)    147584      activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_76 (Add)                    (None, 8, 8, 128)    0           activation_157[0][0]             \n",
      "                                                                 conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 8, 8, 128)    512         add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 8, 8, 128)    0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 4, 4, 256)    295168      activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 4, 4, 256)    1024        conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 4, 4, 256)    0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 4, 4, 256)    295168      activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 4, 4, 256)    590080      activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_77 (Add)                    (None, 4, 4, 256)    0           conv2d_172[0][0]                 \n",
      "                                                                 conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 4, 4, 256)    1024        add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 4, 4, 256)    0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 4, 4, 256)    590080      activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 4, 4, 256)    1024        conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 4, 4, 256)    0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 4, 4, 256)    590080      activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_78 (Add)                    (None, 4, 4, 256)    0           activation_161[0][0]             \n",
      "                                                                 conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 4, 4, 256)    1024        add_78[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 4, 4, 256)    0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 4, 4, 256)    590080      activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 4, 4, 256)    1024        conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 4, 4, 256)    0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 4, 4, 256)    590080      activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_79 (Add)                    (None, 4, 4, 256)    0           activation_163[0][0]             \n",
      "                                                                 conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 4, 4, 256)    1024        add_79[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 4, 4, 256)    0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 4, 4, 256)    590080      activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 4, 4, 256)    1024        conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 4, 4, 256)    0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 4, 4, 256)    590080      activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_80 (Add)                    (None, 4, 4, 256)    0           activation_165[0][0]             \n",
      "                                                                 conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 4, 4, 256)    1024        add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 4, 4, 256)    0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 4, 4, 256)    590080      activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 4, 4, 256)    1024        conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 4, 4, 256)    0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 4, 4, 256)    590080      activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_81 (Add)                    (None, 4, 4, 256)    0           activation_167[0][0]             \n",
      "                                                                 conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 4, 4, 256)    1024        add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 4, 4, 256)    0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 2, 2, 512)    1180160     activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 2, 2, 512)    2048        conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 2, 2, 512)    0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 2, 2, 512)    1180160     activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 2, 2, 512)    2359808     activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_82 (Add)                    (None, 2, 2, 512)    0           conv2d_183[0][0]                 \n",
      "                                                                 conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 2, 2, 512)    2048        add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 2, 2, 512)    0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 2, 2, 512)    2359808     activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 2, 2, 512)    2048        conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 2, 2, 512)    0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 2, 2, 512)    2359808     activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_83 (Add)                    (None, 2, 2, 512)    0           activation_171[0][0]             \n",
      "                                                                 conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 2, 2, 512)    2048        add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 2, 2, 512)    0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 2048)         0           activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 2048)         0           flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "fc10 (Dense)                    (None, 10)           20490       dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,019,274\n",
      "Trainable params: 17,006,858\n",
      "Non-trainable params: 12,416\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = build_resnet()\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 421s 335ms/step - loss: 2.5958 - acc: 0.1303 - val_loss: 9.7224 - val_acc: 0.1114\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 411s 329ms/step - loss: 2.0030 - acc: 0.2344 - val_loss: 10.5875 - val_acc: 0.1604\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 388s 310ms/step - loss: 2.0408 - acc: 0.2338 - val_loss: 47.4849 - val_acc: 0.1672\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 388s 310ms/step - loss: 1.9013 - acc: 0.2690 - val_loss: 2.9498 - val_acc: 0.1987\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 390s 312ms/step - loss: 1.8471 - acc: 0.2935 - val_loss: 6.0383 - val_acc: 0.2178\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 405s 324ms/step - loss: 1.7885 - acc: 0.3022 - val_loss: 2.6981 - val_acc: 0.1773\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 401s 321ms/step - loss: 1.7378 - acc: 0.3256 - val_loss: 3.6083 - val_acc: 0.2145\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 431s 345ms/step - loss: 1.6751 - acc: 0.3446 - val_loss: 1.8363 - val_acc: 0.2866\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 417s 334ms/step - loss: 1.5872 - acc: 0.3864 - val_loss: 1.6798 - val_acc: 0.3910\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 429s 343ms/step - loss: 1.4988 - acc: 0.4317 - val_loss: 1.8509 - val_acc: 0.3320\n"
     ]
    }
   ],
   "source": [
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate * 100),\n",
    "              metrics=['acc'])\n",
    "history3 = model3.fit(x_tr, y_tr, batch_size=32, epochs=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate 5 * 10^-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNetDropOut0\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPadding2D (None, 38, 38, 3)    0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 32, 32, 64)   9472        zero_padding2d_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 32, 32, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 32, 32, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 15, 15, 64)   0           activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 15, 15, 64)   36928       max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 15, 15, 64)   256         conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 15, 15, 64)   0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 15, 15, 64)   36928       activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_56 (Add)                    (None, 15, 15, 64)   0           max_pooling2d_20[0][0]           \n",
      "                                                                 conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 15, 15, 64)   256         add_56[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 15, 15, 64)   0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 15, 15, 64)   36928       activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 15, 15, 64)   256         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 15, 15, 64)   0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 15, 15, 64)   36928       activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_57 (Add)                    (None, 15, 15, 64)   0           activation_134[0][0]             \n",
      "                                                                 conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 15, 15, 64)   256         add_57[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 15, 15, 64)   0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 8, 8, 128)    73856       activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 8, 8, 128)    512         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 8, 8, 128)    0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 8, 8, 128)    73856       activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 8, 8, 128)    147584      activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_58 (Add)                    (None, 8, 8, 128)    0           conv2d_154[0][0]                 \n",
      "                                                                 conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 8, 8, 128)    512         add_58[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 8, 8, 128)    0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 8, 8, 128)    147584      activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 8, 8, 128)    512         conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 8, 8, 128)    0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 8, 8, 128)    147584      activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_59 (Add)                    (None, 8, 8, 128)    0           activation_138[0][0]             \n",
      "                                                                 conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 8, 8, 128)    512         add_59[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 8, 8, 128)    0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 8, 8, 128)    147584      activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 8, 8, 128)    512         conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 8, 8, 128)    0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 8, 8, 128)    147584      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_60 (Add)                    (None, 8, 8, 128)    0           activation_140[0][0]             \n",
      "                                                                 conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 8, 8, 128)    512         add_60[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 8, 8, 128)    0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 8, 8, 128)    147584      activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 8, 8, 128)    512         conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 8, 8, 128)    0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 8, 8, 128)    147584      activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_61 (Add)                    (None, 8, 8, 128)    0           activation_142[0][0]             \n",
      "                                                                 conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 8, 8, 128)    512         add_61[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 8, 8, 128)    0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 8, 8, 128)    147584      activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 8, 8, 128)    512         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 8, 8, 128)    0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 8, 8, 128)    147584      activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_62 (Add)                    (None, 8, 8, 128)    0           activation_144[0][0]             \n",
      "                                                                 conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 8, 8, 128)    512         add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 8, 8, 128)    0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 4, 4, 256)    295168      activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 4, 4, 256)    1024        conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 4, 4, 256)    0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 4, 4, 256)    295168      activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 4, 4, 256)    590080      activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_63 (Add)                    (None, 4, 4, 256)    0           conv2d_165[0][0]                 \n",
      "                                                                 conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 4, 4, 256)    1024        add_63[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 4, 4, 256)    0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 4, 4, 256)    590080      activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 4, 4, 256)    1024        conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 4, 4, 256)    0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 4, 4, 256)    590080      activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_64 (Add)                    (None, 4, 4, 256)    0           activation_148[0][0]             \n",
      "                                                                 conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 4, 4, 256)    1024        add_64[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 4, 4, 256)    0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 4, 4, 256)    590080      activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 4, 4, 256)    1024        conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 4, 4, 256)    0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 4, 4, 256)    590080      activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_65 (Add)                    (None, 4, 4, 256)    0           activation_150[0][0]             \n",
      "                                                                 conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 4, 4, 256)    1024        add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 4, 4, 256)    0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 4, 4, 256)    590080      activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 4, 4, 256)    1024        conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 4, 4, 256)    0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 4, 4, 256)    590080      activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_66 (Add)                    (None, 4, 4, 256)    0           activation_152[0][0]             \n",
      "                                                                 conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 4, 4, 256)    1024        add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 4, 4, 256)    0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 4, 4, 256)    590080      activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 4, 4, 256)    1024        conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 4, 4, 256)    0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 4, 4, 256)    590080      activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_67 (Add)                    (None, 4, 4, 256)    0           activation_154[0][0]             \n",
      "                                                                 conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 4, 4, 256)    1024        add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 4, 4, 256)    0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 2, 2, 256)    590080      activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 2, 2, 256)    1024        conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 2, 2, 256)    0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 2, 2, 256)    590080      activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 2, 2, 256)    590080      activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_68 (Add)                    (None, 2, 2, 256)    0           conv2d_176[0][0]                 \n",
      "                                                                 conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 2, 2, 256)    1024        add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 2, 2, 256)    0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 2, 2, 256)    590080      activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 2, 2, 256)    1024        conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 2, 2, 256)    0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 2, 2, 256)    590080      activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_69 (Add)                    (None, 2, 2, 256)    0           activation_158[0][0]             \n",
      "                                                                 conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 2, 2, 256)    1024        add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 2, 2, 256)    0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 1024)         0           activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 1024)         0           flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc10 (Dense)                    (None, 10)           10250       dropout_12[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 10,515,594\n",
      "Trainable params: 10,505,226\n",
      "Non-trainable params: 10,368\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4 = build_resnet()\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 219s 173ms/step - loss: 2.0283 - acc: 0.2853 - val_loss: 1.5359 - val_acc: 0.4525\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 219s 175ms/step - loss: 1.3083 - acc: 0.5265 - val_loss: 1.3959 - val_acc: 0.5006\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 227s 182ms/step - loss: 1.0160 - acc: 0.6349 - val_loss: 1.3096 - val_acc: 0.5482\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 225s 180ms/step - loss: 0.7646 - acc: 0.7309 - val_loss: 1.3223 - val_acc: 0.5722\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 219s 175ms/step - loss: 0.5400 - acc: 0.8113 - val_loss: 1.3709 - val_acc: 0.5899\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 218s 175ms/step - loss: 0.3680 - acc: 0.8714 - val_loss: 1.7314 - val_acc: 0.5652\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 217s 174ms/step - loss: 0.2715 - acc: 0.9062 - val_loss: 1.8190 - val_acc: 0.5739\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 220s 176ms/step - loss: 0.2358 - acc: 0.9178 - val_loss: 1.7273 - val_acc: 0.5760\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 215s 172ms/step - loss: 0.1983 - acc: 0.9305 - val_loss: 2.1243 - val_acc: 0.5817\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 215s 172ms/step - loss: 0.1732 - acc: 0.9391 - val_loss: 2.0290 - val_acc: 0.5844\n"
     ]
    }
   ],
   "source": [
    "model4.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate * 5),\n",
    "              metrics=['acc'])\n",
    "history4 = model4.fit(x_tr, y_tr, batch_size=32, epochs=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Dropout\n",
    "\n",
    "Adding dropout increased the validation accuracy and decreased the training accuracy, but the model is still overfit. Next I will add data augmentation.\n",
    "\n",
    "10^-4 \n",
    "training acc: 0.9458 \n",
    "val acc: 0.6327\n",
    "\n",
    "10^-4 with dropout\n",
    "training acc: 0.8292 \n",
    "val acc: 0.7092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNetDropOut50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_26 (ZeroPadding2 (None, 38, 38, 3)    0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 32, 32, 64)   9472        zero_padding2d_26[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 32, 32, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_293 (Activation)     (None, 32, 32, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 15, 15, 64)   0           activation_293[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_295 (Conv2D)             (None, 15, 15, 64)   36928       max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_268 (BatchN (None, 15, 15, 64)   256         conv2d_295[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_294 (Activation)     (None, 15, 15, 64)   0           batch_normalization_268[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_296 (Conv2D)             (None, 15, 15, 64)   36928       activation_294[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_138 (Add)                   (None, 15, 15, 64)   0           max_pooling2d_14[0][0]           \n",
      "                                                                 conv2d_296[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_269 (BatchN (None, 15, 15, 64)   256         add_138[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_295 (Activation)     (None, 15, 15, 64)   0           batch_normalization_269[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_297 (Conv2D)             (None, 15, 15, 64)   36928       activation_295[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_270 (BatchN (None, 15, 15, 64)   256         conv2d_297[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_296 (Activation)     (None, 15, 15, 64)   0           batch_normalization_270[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_298 (Conv2D)             (None, 15, 15, 64)   36928       activation_296[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_139 (Add)                   (None, 15, 15, 64)   0           activation_295[0][0]             \n",
      "                                                                 conv2d_298[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_271 (BatchN (None, 15, 15, 64)   256         add_139[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_297 (Activation)     (None, 15, 15, 64)   0           batch_normalization_271[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_299 (Conv2D)             (None, 8, 8, 128)    73856       activation_297[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_272 (BatchN (None, 8, 8, 128)    512         conv2d_299[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_298 (Activation)     (None, 8, 8, 128)    0           batch_normalization_272[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_301 (Conv2D)             (None, 8, 8, 128)    73856       activation_297[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_300 (Conv2D)             (None, 8, 8, 128)    147584      activation_298[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_140 (Add)                   (None, 8, 8, 128)    0           conv2d_301[0][0]                 \n",
      "                                                                 conv2d_300[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_273 (BatchN (None, 8, 8, 128)    512         add_140[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_299 (Activation)     (None, 8, 8, 128)    0           batch_normalization_273[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_302 (Conv2D)             (None, 8, 8, 128)    147584      activation_299[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_274 (BatchN (None, 8, 8, 128)    512         conv2d_302[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_300 (Activation)     (None, 8, 8, 128)    0           batch_normalization_274[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_303 (Conv2D)             (None, 8, 8, 128)    147584      activation_300[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_141 (Add)                   (None, 8, 8, 128)    0           activation_299[0][0]             \n",
      "                                                                 conv2d_303[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_275 (BatchN (None, 8, 8, 128)    512         add_141[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_301 (Activation)     (None, 8, 8, 128)    0           batch_normalization_275[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_304 (Conv2D)             (None, 8, 8, 128)    147584      activation_301[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_276 (BatchN (None, 8, 8, 128)    512         conv2d_304[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_302 (Activation)     (None, 8, 8, 128)    0           batch_normalization_276[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_305 (Conv2D)             (None, 8, 8, 128)    147584      activation_302[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_142 (Add)                   (None, 8, 8, 128)    0           activation_301[0][0]             \n",
      "                                                                 conv2d_305[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_277 (BatchN (None, 8, 8, 128)    512         add_142[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_303 (Activation)     (None, 8, 8, 128)    0           batch_normalization_277[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_306 (Conv2D)             (None, 8, 8, 128)    147584      activation_303[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_278 (BatchN (None, 8, 8, 128)    512         conv2d_306[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_304 (Activation)     (None, 8, 8, 128)    0           batch_normalization_278[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_307 (Conv2D)             (None, 8, 8, 128)    147584      activation_304[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_143 (Add)                   (None, 8, 8, 128)    0           activation_303[0][0]             \n",
      "                                                                 conv2d_307[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_279 (BatchN (None, 8, 8, 128)    512         add_143[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_305 (Activation)     (None, 8, 8, 128)    0           batch_normalization_279[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_308 (Conv2D)             (None, 8, 8, 128)    147584      activation_305[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_280 (BatchN (None, 8, 8, 128)    512         conv2d_308[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_306 (Activation)     (None, 8, 8, 128)    0           batch_normalization_280[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_309 (Conv2D)             (None, 8, 8, 128)    147584      activation_306[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_144 (Add)                   (None, 8, 8, 128)    0           activation_305[0][0]             \n",
      "                                                                 conv2d_309[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_281 (BatchN (None, 8, 8, 128)    512         add_144[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_307 (Activation)     (None, 8, 8, 128)    0           batch_normalization_281[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_310 (Conv2D)             (None, 4, 4, 256)    295168      activation_307[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_282 (BatchN (None, 4, 4, 256)    1024        conv2d_310[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_308 (Activation)     (None, 4, 4, 256)    0           batch_normalization_282[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_312 (Conv2D)             (None, 4, 4, 256)    295168      activation_307[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_311 (Conv2D)             (None, 4, 4, 256)    590080      activation_308[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_145 (Add)                   (None, 4, 4, 256)    0           conv2d_312[0][0]                 \n",
      "                                                                 conv2d_311[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_283 (BatchN (None, 4, 4, 256)    1024        add_145[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_309 (Activation)     (None, 4, 4, 256)    0           batch_normalization_283[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_313 (Conv2D)             (None, 4, 4, 256)    590080      activation_309[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_284 (BatchN (None, 4, 4, 256)    1024        conv2d_313[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_310 (Activation)     (None, 4, 4, 256)    0           batch_normalization_284[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_314 (Conv2D)             (None, 4, 4, 256)    590080      activation_310[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_146 (Add)                   (None, 4, 4, 256)    0           activation_309[0][0]             \n",
      "                                                                 conv2d_314[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_285 (BatchN (None, 4, 4, 256)    1024        add_146[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_311 (Activation)     (None, 4, 4, 256)    0           batch_normalization_285[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_315 (Conv2D)             (None, 4, 4, 256)    590080      activation_311[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_286 (BatchN (None, 4, 4, 256)    1024        conv2d_315[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_312 (Activation)     (None, 4, 4, 256)    0           batch_normalization_286[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_316 (Conv2D)             (None, 4, 4, 256)    590080      activation_312[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_147 (Add)                   (None, 4, 4, 256)    0           activation_311[0][0]             \n",
      "                                                                 conv2d_316[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_287 (BatchN (None, 4, 4, 256)    1024        add_147[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_313 (Activation)     (None, 4, 4, 256)    0           batch_normalization_287[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_317 (Conv2D)             (None, 4, 4, 256)    590080      activation_313[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_288 (BatchN (None, 4, 4, 256)    1024        conv2d_317[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_314 (Activation)     (None, 4, 4, 256)    0           batch_normalization_288[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_318 (Conv2D)             (None, 4, 4, 256)    590080      activation_314[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_148 (Add)                   (None, 4, 4, 256)    0           activation_313[0][0]             \n",
      "                                                                 conv2d_318[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_289 (BatchN (None, 4, 4, 256)    1024        add_148[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_315 (Activation)     (None, 4, 4, 256)    0           batch_normalization_289[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_319 (Conv2D)             (None, 4, 4, 256)    590080      activation_315[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_290 (BatchN (None, 4, 4, 256)    1024        conv2d_319[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_316 (Activation)     (None, 4, 4, 256)    0           batch_normalization_290[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_320 (Conv2D)             (None, 4, 4, 256)    590080      activation_316[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_149 (Add)                   (None, 4, 4, 256)    0           activation_315[0][0]             \n",
      "                                                                 conv2d_320[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_291 (BatchN (None, 4, 4, 256)    1024        add_149[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_317 (Activation)     (None, 4, 4, 256)    0           batch_normalization_291[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_321 (Conv2D)             (None, 2, 2, 512)    1180160     activation_317[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_292 (BatchN (None, 2, 2, 512)    2048        conv2d_321[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_318 (Activation)     (None, 2, 2, 512)    0           batch_normalization_292[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_323 (Conv2D)             (None, 2, 2, 512)    1180160     activation_317[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_322 (Conv2D)             (None, 2, 2, 512)    2359808     activation_318[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_150 (Add)                   (None, 2, 2, 512)    0           conv2d_323[0][0]                 \n",
      "                                                                 conv2d_322[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_293 (BatchN (None, 2, 2, 512)    2048        add_150[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_319 (Activation)     (None, 2, 2, 512)    0           batch_normalization_293[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_324 (Conv2D)             (None, 2, 2, 512)    2359808     activation_319[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_294 (BatchN (None, 2, 2, 512)    2048        conv2d_324[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_320 (Activation)     (None, 2, 2, 512)    0           batch_normalization_294[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_325 (Conv2D)             (None, 2, 2, 512)    2359808     activation_320[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_151 (Add)                   (None, 2, 2, 512)    0           activation_319[0][0]             \n",
      "                                                                 conv2d_325[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_295 (BatchN (None, 2, 2, 512)    2048        add_151[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_321 (Activation)     (None, 2, 2, 512)    0           batch_normalization_295[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 2048)         0           activation_321[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 2048)         0           flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc10 (Dense)                    (None, 10)           20490       dropout_12[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 17,019,274\n",
      "Trainable params: 17,006,858\n",
      "Non-trainable params: 12,416\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5 = build_resnet(0.5)\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 433s 346ms/step - loss: 0.7172 - acc: 0.7544 - val_loss: 1.1310 - val_acc: 0.6168\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 427s 342ms/step - loss: 0.7079 - acc: 0.7628 - val_loss: 3.0078 - val_acc: 0.6635\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 419s 335ms/step - loss: 0.6858 - acc: 0.7673 - val_loss: 3.0673 - val_acc: 0.6018\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 435s 348ms/step - loss: 0.6571 - acc: 0.7814 - val_loss: 3.9690 - val_acc: 0.6992\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 438s 351ms/step - loss: 0.6365 - acc: 0.7857 - val_loss: 0.9552 - val_acc: 0.7129\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 429s 343ms/step - loss: 0.6192 - acc: 0.7962 - val_loss: 1.4254 - val_acc: 0.6107\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 441s 353ms/step - loss: 0.5939 - acc: 0.8047 - val_loss: 1.0457 - val_acc: 0.7004\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 425s 340ms/step - loss: 0.5612 - acc: 0.8148 - val_loss: 0.8777 - val_acc: 0.7237\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 428s 342ms/step - loss: 0.5365 - acc: 0.8220 - val_loss: 2.2571 - val_acc: 0.6169\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 444s 355ms/step - loss: 0.5222 - acc: 0.8292 - val_loss: 0.9934 - val_acc: 0.7092\n"
     ]
    }
   ],
   "source": [
    "model5.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate * 10),\n",
    "              metrics=['acc'])\n",
    "history5 = model5.fit(x_tr, y_tr, batch_size=32, epochs=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "First I reduced the number of convolutions to reduce the time it takes to run the network.\n",
    "The result was:\n",
    "acc: 0.9528\n",
    "val acc: 0.6719\n",
    "\n",
    "Data augmentation is implemented using ImageDataGenerator and the following networks run with this augmented data.\n",
    "\n",
    "I tried to make a smaller network since the current one takes way to long to train, but it was not as good even with data augmentation.\n",
    "After 20 epochs:\n",
    "acc: 0.5664\n",
    "val acc: 0.6481\n",
    "\n",
    "Since I dont have time to play around with this smaller network, I plan on fixing the hyper-parameters after the due-date of the assignment for personal gain.\n",
    "\n",
    "Going back to the larger, resnet based network, I noticed a that the dropout layer was right before the only Dense layer. To fix this, I added another Dense layer, making sure to move the dropout layer to before that layer as well. I ran this network for 50 epochs and the result is:\n",
    "acc: 0.8265\n",
    "val_acc: 0.7688\n",
    "\n",
    "Although this is a notible improvement, and the network is not as overfitted as the past networks, there is still work to be done. I believe the main problem is that there are too many parameters for the reletively small training set. This 18 layer network with over 3 million parameters is way to complex for the 40,000 training samples. The smaller 6 layer network with only a quarter of a million parameters should be a lot closer in complexity for the 40,000 training samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNetDropOut50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 38, 38, 3)    0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 32, 32, 64)   9472        zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 32, 32, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 32, 32, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 15, 15, 64)   0           activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 15, 15, 64)   36928       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 15, 15, 64)   256         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 15, 15, 64)   0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 15, 15, 64)   36928       activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 15, 15, 64)   0           max_pooling2d_4[0][0]            \n",
      "                                                                 conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 15, 15, 64)   256         add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 15, 15, 64)   0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 15, 15, 64)   36928       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 15, 15, 64)   256         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 15, 15, 64)   0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 15, 15, 64)   36928       activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 15, 15, 64)   0           activation_70[0][0]              \n",
      "                                                                 conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 15, 15, 64)   256         add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 15, 15, 64)   0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 8, 128)    73856       activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 8, 8, 128)    512         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 8, 8, 128)    0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 8, 8, 128)    73856       activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 8, 8, 128)    147584      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 8, 8, 128)    0           conv2d_82[0][0]                  \n",
      "                                                                 conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 8, 8, 128)    512         add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 8, 8, 128)    0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 8, 8, 128)    147584      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 8, 8, 128)    512         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 8, 8, 128)    0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 8, 8, 128)    147584      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 8, 8, 128)    0           activation_74[0][0]              \n",
      "                                                                 conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 8, 8, 128)    512         add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 8, 8, 128)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 4, 4, 256)    295168      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 4, 4, 256)    1024        conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 4, 4, 256)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 4, 4, 256)    295168      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 4, 4, 256)    590080      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 4, 4, 256)    0           conv2d_87[0][0]                  \n",
      "                                                                 conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 4, 4, 256)    1024        add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 4, 4, 256)    0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 2, 2, 256)    590080      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 2, 2, 256)    1024        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 2, 2, 256)    0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 2, 2, 256)    590080      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 2, 2, 256)    590080      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 2, 2, 256)    0           conv2d_90[0][0]                  \n",
      "                                                                 conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 2, 2, 256)    1024        add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 2, 2, 256)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 1024)         0           activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1024)         0           flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "fc10 (Dense)                    (None, 10)           10250       dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,715,978\n",
      "Trainable params: 3,712,266\n",
      "Non-trainable params: 3,712\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model6 = build_resnet(dropout=0.5, num_blocks_list=[2,2,1,1])\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 120s 95ms/step - loss: 1.9604 - acc: 0.3428 - val_loss: 1.4327 - val_acc: 0.5089\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 115s 92ms/step - loss: 1.1807 - acc: 0.5881 - val_loss: 1.3576 - val_acc: 0.5348\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 115s 92ms/step - loss: 0.8744 - acc: 0.6929 - val_loss: 1.1988 - val_acc: 0.6196\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 115s 92ms/step - loss: 0.6442 - acc: 0.7755 - val_loss: 1.4256 - val_acc: 0.5812\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 115s 92ms/step - loss: 0.4552 - acc: 0.8429 - val_loss: 1.3442 - val_acc: 0.6188\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 115s 92ms/step - loss: 0.3133 - acc: 0.8928 - val_loss: 1.6278 - val_acc: 0.6336\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 125s 100ms/step - loss: 0.2352 - acc: 0.9185 - val_loss: 1.2669 - val_acc: 0.6825\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 124s 99ms/step - loss: 0.1834 - acc: 0.9380 - val_loss: 2.1755 - val_acc: 0.6328\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 121s 96ms/step - loss: 0.1607 - acc: 0.9482 - val_loss: 2.8408 - val_acc: 0.5686\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 118s 95ms/step - loss: 0.1420 - acc: 0.9528 - val_loss: 1.6272 - val_acc: 0.6719\n"
     ]
    }
   ],
   "source": [
    "model6.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate * 10),\n",
    "              metrics=['acc'])\n",
    "history6 = model6.fit(x_tr, y_tr, batch_size=32, epochs=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "datagen.fit(x_tr, augment=True)\n",
    "\n",
    "val_gen = ImageDataGenerator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_small_cnn(num_classes=10,dropout = 0.0):\n",
    "    X_input = Input(shape=(32,32,3))\n",
    "    \n",
    "#             layers.experimental.preprocessing.Sequential([RandomFlip('horizontal'),\n",
    "#             layers.experimental.preprocessing.RandomRotation(20),\n",
    "#             layers.experimental.preprocessing.RandomContrast(factor=0.2),\n",
    "#             layers.experimental.preprocessing.RandomWidth(0.2),\n",
    "#             layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "\n",
    "    X = Conv2D(filters = 64, kernel_size=(7,7), strides=(1,1), name = 'conv1', kernel_initializer=glorot_uniform(seed=0))(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(64, 3)(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(64, 3)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3,3))(X)\n",
    "    X = Conv2D(128, 3)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(128, 3)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = AveragePooling2D((4,4))(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dropout(dropout)(X)\n",
    "    X = Dense(128, activation='relu', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = Dense(num_classes, activation='softmax', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 30, 30, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 5, 5, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 5, 5, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 279,498\n",
      "Trainable params: 278,730\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "312/312 [==============================] - 57s 182ms/step - loss: 2.1240 - acc: 0.2235 - val_loss: 1.6650 - val_acc: 0.4073\n",
      "Epoch 2/20\n",
      "312/312 [==============================] - 55s 177ms/step - loss: 1.7581 - acc: 0.3591 - val_loss: 1.4922 - val_acc: 0.4694\n",
      "Epoch 3/20\n",
      "312/312 [==============================] - 54s 174ms/step - loss: 1.6403 - acc: 0.4006 - val_loss: 1.3869 - val_acc: 0.4939\n",
      "Epoch 4/20\n",
      "312/312 [==============================] - 55s 176ms/step - loss: 1.5720 - acc: 0.4326 - val_loss: 1.3442 - val_acc: 0.5053\n",
      "Epoch 5/20\n",
      "312/312 [==============================] - 56s 178ms/step - loss: 1.5110 - acc: 0.4529 - val_loss: 1.3015 - val_acc: 0.5290\n",
      "Epoch 6/20\n",
      "312/312 [==============================] - 56s 178ms/step - loss: 1.4766 - acc: 0.4684 - val_loss: 1.4348 - val_acc: 0.4929\n",
      "Epoch 7/20\n",
      "312/312 [==============================] - 56s 178ms/step - loss: 1.4431 - acc: 0.4845 - val_loss: 1.6285 - val_acc: 0.4744\n",
      "Epoch 8/20\n",
      "312/312 [==============================] - 54s 174ms/step - loss: 1.4241 - acc: 0.4890 - val_loss: 1.2950 - val_acc: 0.5356\n",
      "Epoch 9/20\n",
      "312/312 [==============================] - 54s 174ms/step - loss: 1.3953 - acc: 0.4989 - val_loss: 1.1989 - val_acc: 0.5702\n",
      "Epoch 10/20\n",
      "312/312 [==============================] - 54s 174ms/step - loss: 1.3673 - acc: 0.5116 - val_loss: 1.2117 - val_acc: 0.5606\n",
      "Epoch 11/20\n",
      "312/312 [==============================] - 55s 175ms/step - loss: 1.3474 - acc: 0.5239 - val_loss: 1.1548 - val_acc: 0.5788\n",
      "Epoch 12/20\n",
      "312/312 [==============================] - 54s 175ms/step - loss: 1.3285 - acc: 0.5269 - val_loss: 1.2626 - val_acc: 0.5560\n",
      "Epoch 13/20\n",
      "312/312 [==============================] - 54s 174ms/step - loss: 1.3099 - acc: 0.5341 - val_loss: 1.2170 - val_acc: 0.5734\n",
      "Epoch 14/20\n",
      "312/312 [==============================] - 54s 175ms/step - loss: 1.2920 - acc: 0.5448 - val_loss: 1.1864 - val_acc: 0.5886\n",
      "Epoch 15/20\n",
      "312/312 [==============================] - 54s 174ms/step - loss: 1.2907 - acc: 0.5460 - val_loss: 1.1611 - val_acc: 0.5831\n",
      "Epoch 16/20\n",
      "312/312 [==============================] - 58s 185ms/step - loss: 1.2660 - acc: 0.5533 - val_loss: 1.1195 - val_acc: 0.6014\n",
      "Epoch 17/20\n",
      "312/312 [==============================] - 55s 175ms/step - loss: 1.2545 - acc: 0.5558 - val_loss: 1.0673 - val_acc: 0.6220\n",
      "Epoch 18/20\n",
      "312/312 [==============================] - 59s 190ms/step - loss: 1.2364 - acc: 0.5621 - val_loss: 0.9861 - val_acc: 0.6441\n",
      "Epoch 19/20\n",
      "312/312 [==============================] - 57s 181ms/step - loss: 1.2474 - acc: 0.5608 - val_loss: 1.1584 - val_acc: 0.5842\n",
      "Epoch 20/20\n",
      "312/312 [==============================] - 54s 173ms/step - loss: 1.2201 - acc: 0.5664 - val_loss: 0.9983 - val_acc: 0.6481\n"
     ]
    }
   ],
   "source": [
    "model7 = build_small_cnn(dropout=0.5)\n",
    "batch_size = 128\n",
    "print(model7.summary())\n",
    "model7.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate * 10),\n",
    "              metrics=['acc'])\n",
    "history7 = model7.fit(datagen.flow(x_tr, y_tr, batch_size=batch_size), epochs=20, steps_per_epoch=x_tr.shape[0]//batch_size, validation_steps=x_val.shape[0]//batch_size, validation_data=val_gen.flow(x_val, y_val, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "312/312 [==============================] - 112s 353ms/step - loss: 2.1232 - acc: 0.2442 - val_loss: 1.6552 - val_acc: 0.3810\n",
      "Epoch 2/50\n",
      "312/312 [==============================] - 114s 363ms/step - loss: 1.6274 - acc: 0.4050 - val_loss: 1.4526 - val_acc: 0.4876\n",
      "Epoch 3/50\n",
      "312/312 [==============================] - 111s 356ms/step - loss: 1.4805 - acc: 0.4623 - val_loss: 1.3585 - val_acc: 0.5080\n",
      "Epoch 4/50\n",
      "312/312 [==============================] - 111s 357ms/step - loss: 1.3855 - acc: 0.4945 - val_loss: 1.3508 - val_acc: 0.5138\n",
      "Epoch 5/50\n",
      "312/312 [==============================] - 111s 357ms/step - loss: 1.3072 - acc: 0.5266 - val_loss: 1.2143 - val_acc: 0.5791\n",
      "Epoch 6/50\n",
      "312/312 [==============================] - 112s 357ms/step - loss: 1.2387 - acc: 0.5559 - val_loss: 1.1047 - val_acc: 0.6099\n",
      "Epoch 7/50\n",
      "312/312 [==============================] - 111s 357ms/step - loss: 1.1794 - acc: 0.5793 - val_loss: 1.3246 - val_acc: 0.5360\n",
      "Epoch 8/50\n",
      "312/312 [==============================] - 111s 357ms/step - loss: 1.1485 - acc: 0.5920 - val_loss: 1.1664 - val_acc: 0.5985\n",
      "Epoch 9/50\n",
      "312/312 [==============================] - 111s 357ms/step - loss: 1.0903 - acc: 0.6112 - val_loss: 1.0316 - val_acc: 0.6324\n",
      "Epoch 10/50\n",
      "312/312 [==============================] - 112s 357ms/step - loss: 1.0644 - acc: 0.6229 - val_loss: 0.9719 - val_acc: 0.6623\n",
      "Epoch 11/50\n",
      "312/312 [==============================] - 112s 358ms/step - loss: 1.0223 - acc: 0.6362 - val_loss: 1.0955 - val_acc: 0.6179\n",
      "Epoch 12/50\n",
      "312/312 [==============================] - 112s 359ms/step - loss: 0.9832 - acc: 0.6520 - val_loss: 1.3085 - val_acc: 0.5939\n",
      "Epoch 13/50\n",
      "312/312 [==============================] - 110s 353ms/step - loss: 0.9618 - acc: 0.6611 - val_loss: 0.9474 - val_acc: 0.6773\n",
      "Epoch 14/50\n",
      "312/312 [==============================] - 113s 363ms/step - loss: 0.9388 - acc: 0.6700 - val_loss: 0.8767 - val_acc: 0.6980\n",
      "Epoch 15/50\n",
      "312/312 [==============================] - 117s 374ms/step - loss: 0.8995 - acc: 0.6840 - val_loss: 0.8890 - val_acc: 0.6910\n",
      "Epoch 16/50\n",
      "312/312 [==============================] - 112s 359ms/step - loss: 0.8719 - acc: 0.6919 - val_loss: 1.1316 - val_acc: 0.6422\n",
      "Epoch 17/50\n",
      "312/312 [==============================] - 111s 356ms/step - loss: 0.8563 - acc: 0.6994 - val_loss: 0.7884 - val_acc: 0.7319\n",
      "Epoch 18/50\n",
      "312/312 [==============================] - 111s 357ms/step - loss: 0.8397 - acc: 0.7041 - val_loss: 0.8288 - val_acc: 0.7173\n",
      "Epoch 19/50\n",
      "312/312 [==============================] - 118s 378ms/step - loss: 0.8140 - acc: 0.7158 - val_loss: 0.8778 - val_acc: 0.7082\n",
      "Epoch 20/50\n",
      "312/312 [==============================] - 115s 367ms/step - loss: 0.7922 - acc: 0.7226 - val_loss: 0.7964 - val_acc: 0.7252\n",
      "Epoch 21/50\n",
      "312/312 [==============================] - 112s 360ms/step - loss: 0.7674 - acc: 0.7310 - val_loss: 0.7103 - val_acc: 0.7534\n",
      "Epoch 22/50\n",
      "312/312 [==============================] - 112s 358ms/step - loss: 0.7587 - acc: 0.7358 - val_loss: 0.9775 - val_acc: 0.6877\n",
      "Epoch 23/50\n",
      "312/312 [==============================] - 112s 359ms/step - loss: 0.7526 - acc: 0.7357 - val_loss: 0.9092 - val_acc: 0.7036\n",
      "Epoch 24/50\n",
      "312/312 [==============================] - 112s 358ms/step - loss: 0.7314 - acc: 0.7429 - val_loss: 0.7611 - val_acc: 0.7479\n",
      "Epoch 25/50\n",
      "312/312 [==============================] - 112s 360ms/step - loss: 0.7094 - acc: 0.7545 - val_loss: 0.7308 - val_acc: 0.7519\n",
      "Epoch 26/50\n",
      "312/312 [==============================] - 114s 364ms/step - loss: 0.6998 - acc: 0.7552 - val_loss: 0.8739 - val_acc: 0.7125\n",
      "Epoch 27/50\n",
      "312/312 [==============================] - 113s 362ms/step - loss: 0.6944 - acc: 0.7598 - val_loss: 0.7568 - val_acc: 0.7410\n",
      "Epoch 28/50\n",
      "312/312 [==============================] - 112s 360ms/step - loss: 0.6738 - acc: 0.7686 - val_loss: 1.4944 - val_acc: 0.6172\n",
      "Epoch 29/50\n",
      "312/312 [==============================] - 110s 353ms/step - loss: 0.6649 - acc: 0.7716 - val_loss: 0.7520 - val_acc: 0.7532\n",
      "Epoch 30/50\n",
      "312/312 [==============================] - 110s 353ms/step - loss: 0.6540 - acc: 0.7755 - val_loss: 0.8544 - val_acc: 0.7265\n",
      "Epoch 31/50\n",
      "312/312 [==============================] - 108s 348ms/step - loss: 0.6382 - acc: 0.7802 - val_loss: 0.9034 - val_acc: 0.7107\n",
      "Epoch 32/50\n",
      "312/312 [==============================] - 111s 355ms/step - loss: 0.6225 - acc: 0.7834 - val_loss: 0.6567 - val_acc: 0.7832\n",
      "Epoch 33/50\n",
      "312/312 [==============================] - 108s 345ms/step - loss: 0.6147 - acc: 0.7864 - val_loss: 1.2024 - val_acc: 0.6538\n",
      "Epoch 34/50\n",
      "312/312 [==============================] - 105s 336ms/step - loss: 0.6183 - acc: 0.7818 - val_loss: 0.7200 - val_acc: 0.7632\n",
      "Epoch 35/50\n",
      "312/312 [==============================] - 105s 336ms/step - loss: 0.5983 - acc: 0.7892 - val_loss: 0.8177 - val_acc: 0.7495\n",
      "Epoch 36/50\n",
      "312/312 [==============================] - 105s 335ms/step - loss: 0.5859 - acc: 0.7962 - val_loss: 0.7918 - val_acc: 0.7433\n",
      "Epoch 37/50\n",
      "312/312 [==============================] - 104s 333ms/step - loss: 0.5917 - acc: 0.7955 - val_loss: 0.9132 - val_acc: 0.7169\n",
      "Epoch 38/50\n",
      "312/312 [==============================] - 106s 341ms/step - loss: 0.5751 - acc: 0.8011 - val_loss: 0.7273 - val_acc: 0.7703\n",
      "Epoch 39/50\n",
      "312/312 [==============================] - 107s 344ms/step - loss: 0.5608 - acc: 0.8045 - val_loss: 0.7119 - val_acc: 0.7759\n",
      "Epoch 40/50\n",
      "312/312 [==============================] - 107s 343ms/step - loss: 0.5593 - acc: 0.8100 - val_loss: 0.5981 - val_acc: 0.7989\n",
      "Epoch 41/50\n",
      "312/312 [==============================] - 106s 338ms/step - loss: 0.5529 - acc: 0.8071 - val_loss: 0.7043 - val_acc: 0.7731\n",
      "Epoch 42/50\n",
      "312/312 [==============================] - 106s 338ms/step - loss: 0.5455 - acc: 0.8094 - val_loss: 0.6869 - val_acc: 0.7810\n",
      "Epoch 43/50\n",
      "312/312 [==============================] - 106s 339ms/step - loss: 0.5361 - acc: 0.8134 - val_loss: 0.6539 - val_acc: 0.7832\n",
      "Epoch 44/50\n",
      "312/312 [==============================] - 106s 340ms/step - loss: 0.5326 - acc: 0.8180 - val_loss: 0.7574 - val_acc: 0.7594\n",
      "Epoch 45/50\n",
      "312/312 [==============================] - 106s 339ms/step - loss: 0.5260 - acc: 0.8174 - val_loss: 0.7919 - val_acc: 0.7522\n",
      "Epoch 46/50\n",
      "312/312 [==============================] - 106s 339ms/step - loss: 0.5196 - acc: 0.8225 - val_loss: 1.1036 - val_acc: 0.6820\n",
      "Epoch 47/50\n",
      "312/312 [==============================] - 106s 338ms/step - loss: 0.5157 - acc: 0.8215 - val_loss: 0.8017 - val_acc: 0.7587\n",
      "Epoch 48/50\n",
      "312/312 [==============================] - 105s 336ms/step - loss: 0.5223 - acc: 0.8182 - val_loss: 0.5907 - val_acc: 0.8090\n",
      "Epoch 49/50\n",
      "312/312 [==============================] - 105s 336ms/step - loss: 0.4972 - acc: 0.8270 - val_loss: 0.8298 - val_acc: 0.7521\n",
      "Epoch 50/50\n",
      "312/312 [==============================] - 106s 339ms/step - loss: 0.5032 - acc: 0.8265 - val_loss: 0.7239 - val_acc: 0.7688\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "model9 = build_resnet(dropout=0.5, num_blocks_list=[2,2,1,1])\n",
    "model9.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate * 10),\n",
    "              metrics=['acc'])\n",
    "history9 = model9.fit(datagen.flow(x_tr, y_tr, batch_size=batch_size), epochs=50, steps_per_epoch=x_tr.shape[0]//batch_size, validation_steps=x_val.shape[0]//batch_size, validation_data=val_gen.flow(x_val, y_val, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history9.history['acc']\n",
    "val_acc = history9.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train (again) and evaluate the model\n",
    "\n",
    "- To this end, you have found the \"best\" hyper-parameters. \n",
    "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
    "- Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "312/312 [==============================] - 102s 321ms/step - loss: 2.1156 - acc: 0.2501\n",
      "Epoch 2/50\n",
      "312/312 [==============================] - 100s 321ms/step - loss: 1.6328 - acc: 0.4024\n",
      "Epoch 3/50\n",
      "312/312 [==============================] - 99s 318ms/step - loss: 1.4951 - acc: 0.4551\n",
      "Epoch 4/50\n",
      "312/312 [==============================] - 100s 321ms/step - loss: 1.3957 - acc: 0.4928\n",
      "Epoch 5/50\n",
      "312/312 [==============================] - 102s 328ms/step - loss: 1.2909 - acc: 0.5366\n",
      "Epoch 6/50\n",
      "312/312 [==============================] - 103s 329ms/step - loss: 1.2370 - acc: 0.5575\n",
      "Epoch 7/50\n",
      "312/312 [==============================] - 105s 335ms/step - loss: 1.1887 - acc: 0.5764\n",
      "Epoch 8/50\n",
      "312/312 [==============================] - 100s 322ms/step - loss: 1.1459 - acc: 0.5912\n",
      "Epoch 9/50\n",
      "312/312 [==============================] - 100s 322ms/step - loss: 1.0983 - acc: 0.6091\n",
      "Epoch 10/50\n",
      "312/312 [==============================] - 100s 320ms/step - loss: 1.0518 - acc: 0.6278\n",
      "Epoch 11/50\n",
      "312/312 [==============================] - 102s 326ms/step - loss: 1.0299 - acc: 0.6408\n",
      "Epoch 12/50\n",
      "312/312 [==============================] - 102s 328ms/step - loss: 0.9876 - acc: 0.6514\n",
      "Epoch 13/50\n",
      "312/312 [==============================] - 123s 394ms/step - loss: 0.9525 - acc: 0.6618\n",
      "Epoch 14/50\n",
      "312/312 [==============================] - 102s 327ms/step - loss: 0.9317 - acc: 0.6736\n",
      "Epoch 15/50\n",
      "312/312 [==============================] - 114s 365ms/step - loss: 0.8954 - acc: 0.6884\n",
      "Epoch 16/50\n",
      "312/312 [==============================] - 110s 354ms/step - loss: 0.8777 - acc: 0.6961\n",
      "Epoch 17/50\n",
      "312/312 [==============================] - 107s 343ms/step - loss: 0.8602 - acc: 0.6958\n",
      "Epoch 18/50\n",
      "312/312 [==============================] - 107s 344ms/step - loss: 0.8300 - acc: 0.7121\n",
      "Epoch 19/50\n",
      "312/312 [==============================] - 107s 344ms/step - loss: 0.8157 - acc: 0.7162\n",
      "Epoch 20/50\n",
      "312/312 [==============================] - 117s 375ms/step - loss: 0.7991 - acc: 0.7225\n",
      "Epoch 21/50\n",
      "312/312 [==============================] - 111s 356ms/step - loss: 0.7866 - acc: 0.7269\n",
      "Epoch 22/50\n",
      "312/312 [==============================] - 105s 336ms/step - loss: 0.7635 - acc: 0.7329\n",
      "Epoch 23/50\n",
      "312/312 [==============================] - 107s 344ms/step - loss: 0.7505 - acc: 0.7397\n",
      "Epoch 24/50\n",
      "312/312 [==============================] - 110s 352ms/step - loss: 0.7280 - acc: 0.7498\n",
      "Epoch 25/50\n",
      "312/312 [==============================] - 109s 350ms/step - loss: 0.7249 - acc: 0.7473\n",
      "Epoch 26/50\n",
      "312/312 [==============================] - 107s 342ms/step - loss: 0.7084 - acc: 0.7530\n",
      "Epoch 27/50\n",
      "312/312 [==============================] - 104s 333ms/step - loss: 0.6846 - acc: 0.7612\n",
      "Epoch 28/50\n",
      "312/312 [==============================] - 107s 341ms/step - loss: 0.6873 - acc: 0.7641\n",
      "Epoch 29/50\n",
      "312/312 [==============================] - 106s 340ms/step - loss: 0.6799 - acc: 0.7641\n",
      "Epoch 30/50\n",
      "312/312 [==============================] - 106s 340ms/step - loss: 0.6730 - acc: 0.7665\n",
      "Epoch 31/50\n",
      "312/312 [==============================] - 107s 343ms/step - loss: 0.6544 - acc: 0.7721\n",
      "Epoch 32/50\n",
      "312/312 [==============================] - 108s 345ms/step - loss: 0.6556 - acc: 0.7732\n",
      "Epoch 33/50\n",
      "312/312 [==============================] - 113s 363ms/step - loss: 0.6298 - acc: 0.7808\n",
      "Epoch 34/50\n",
      "312/312 [==============================] - 106s 339ms/step - loss: 0.6289 - acc: 0.7838\n",
      "Epoch 35/50\n",
      "312/312 [==============================] - 105s 336ms/step - loss: 0.6217 - acc: 0.7831\n",
      "Epoch 36/50\n",
      "312/312 [==============================] - 103s 330ms/step - loss: 0.6131 - acc: 0.7867\n",
      "Epoch 37/50\n",
      "312/312 [==============================] - 100s 319ms/step - loss: 0.5993 - acc: 0.7929\n",
      "Epoch 38/50\n",
      "312/312 [==============================] - 103s 330ms/step - loss: 0.5900 - acc: 0.7953\n",
      "Epoch 39/50\n",
      "312/312 [==============================] - 102s 328ms/step - loss: 0.5836 - acc: 0.7990\n",
      "Epoch 40/50\n",
      "312/312 [==============================] - 108s 345ms/step - loss: 0.5878 - acc: 0.7951\n",
      "Epoch 41/50\n",
      "312/312 [==============================] - 108s 346ms/step - loss: 0.5707 - acc: 0.8047\n",
      "Epoch 42/50\n",
      "312/312 [==============================] - 106s 338ms/step - loss: 0.5647 - acc: 0.8083\n",
      "Epoch 43/50\n",
      "312/312 [==============================] - 108s 347ms/step - loss: 0.5573 - acc: 0.8065\n",
      "Epoch 44/50\n",
      "312/312 [==============================] - 106s 341ms/step - loss: 0.5551 - acc: 0.8082\n",
      "Epoch 45/50\n",
      "312/312 [==============================] - 107s 342ms/step - loss: 0.5413 - acc: 0.8127\n",
      "Epoch 46/50\n",
      "312/312 [==============================] - 107s 343ms/step - loss: 0.5307 - acc: 0.8164\n",
      "Epoch 47/50\n",
      "312/312 [==============================] - 106s 339ms/step - loss: 0.5454 - acc: 0.8132\n",
      "Epoch 48/50\n",
      "312/312 [==============================] - 105s 338ms/step - loss: 0.5378 - acc: 0.8166\n",
      "Epoch 49/50\n",
      "312/312 [==============================] - 107s 342ms/step - loss: 0.5279 - acc: 0.8193\n",
      "Epoch 50/50\n",
      "312/312 [==============================] - 108s 347ms/step - loss: 0.5234 - acc: 0.8189\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen_ad = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "datagen_ad.fit(x_train, augment=True)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "model_all_data = build_resnet(dropout=0.5, num_blocks_list=[2,2,1,1])\n",
    "model_all_data.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate * 10),\n",
    "              metrics=['acc'])\n",
    "history_all_data = model_all_data.fit(datagen_ad.flow(x_train,y_train_vec, batch_size=batch_size), epochs=50, steps_per_epoch=x_tr.shape[0]//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 6s 18ms/step - loss: 0.8377 - acc: 0.7545\n",
      "loss = 0.837701678276062\n",
      "accuracy = 0.7544999718666077\n"
     ]
    }
   ],
   "source": [
    "loss_and_acc = model_all_data.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
